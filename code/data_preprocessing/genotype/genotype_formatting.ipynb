{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "alert-arena",
   "metadata": {
    "kernel": "SoS",
    "tags": []
   },
   "source": [
    "# Genotype data formatting\n",
    "\n",
    "This module implements a collection of workflows used to format genotype data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qualified-there",
   "metadata": {
    "kernel": "SoS",
    "tags": []
   },
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sapphire-elevation",
   "metadata": {
    "kernel": "SoS",
    "tags": []
   },
   "source": [
    "The module streamlines conversion between PLINK and VCF formats (possibly more to add), specifically:\n",
    "\n",
    "1. Conversion between VCF and PLINK formats\n",
    "2. Split data (by specified input, by chromosomes, by genes)\n",
    "3. Merge data (by specified input, by chromosomes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "passive-supplement",
   "metadata": {
    "kernel": "SoS",
    "tags": []
   },
   "source": [
    "## Input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "historic-bahamas",
   "metadata": {
    "kernel": "SoS",
    "tags": []
   },
   "source": [
    "Depending on the analysis task, input files are specified in one of the following formats:\n",
    "\n",
    "1. A single Whole genome data in VCF format, or in PLINK bim/bed/fam bundle; Or,\n",
    "2. A list of VCF or PLINK bed file\n",
    "3. A singular column file containing a list of VCF or PLINK bed file\n",
    "4. A two column file containing a list of per chromosome VCF or PLINK bed file where the first column is chrom and 2nd column is file name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imported-photography",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Output\n",
    "\n",
    "Genotype data after reformatting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "french-rings",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Examples\n",
    "\n",
    "Minimal working example data-set as well as the singularity container `bioinfo.sif` can be downloaded from [Google Drive](https://drive.google.com/drive/u/0/folders/1ahIZGnmjcGwSd-BI91C9ayd_Ya8sB2ed).\n",
    "\n",
    "### PLINK file merger\n",
    "\n",
    "```\n",
    "sos run genotype_formatting.ipynb merge_plink \\\n",
    "    --genoFile data/genotype/chr1.bed data/genotype/chr6.bed \\\n",
    "    --cwd output/genotype \\\n",
    "    --name chr1_chr6 \\\n",
    "    --container container/bioinfo.sif\n",
    "```\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "regulation-hydrogen",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Command interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "scientific-world",
   "metadata": {
    "kernel": "Bash"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: sos run genotype_formatting.ipynb\n",
      "               [workflow_name | -t targets] [options] [workflow_options]\n",
      "  workflow_name:        Single or combined workflows defined in this script\n",
      "  targets:              One or more targets to generate\n",
      "  options:              Single-hyphen sos parameters (see \"sos run -h\" for details)\n",
      "  workflow_options:     Double-hyphen workflow-specific parameters\n",
      "\n",
      "Workflows:\n",
      "  plink_to_vcf\n",
      "  vcf_to_plink\n",
      "  plink_by_gene\n",
      "  plink_by_chrom\n",
      "  merge_plink\n",
      "  merge_vcf\n",
      "\n",
      "Global Workflow Options:\n",
      "  --cwd output (as path)\n",
      "                        Work directory & output directory\n",
      "  --container ''\n",
      "                        The filename name for containers\n",
      "  --job-size 1 (as int)\n",
      "                        For cluster jobs, number commands to run per job\n",
      "  --walltime 5h\n",
      "                        Wall clock time expected\n",
      "  --mem 3G\n",
      "                        Memory expected\n",
      "  --numThreads 20 (as int)\n",
      "                        Number of threads\n",
      "  --genoFile  paths\n",
      "\n",
      "                        the path to a bed file or VCF file, a vector of bed\n",
      "                        files or VCF files, or a text file listing the bed files\n",
      "                        or VCF files to process\n",
      "\n",
      "Sections\n",
      "  plink_to_vcf_1:\n",
      "  vcf_to_plink:\n",
      "  plink_by_gene_1:\n",
      "    Workflow Options:\n",
      "      --window 500000 (as int)\n",
      "                        cis window size\n",
      "      --region-list VAL (as path, required)\n",
      "                        Region definition\n",
      "  plink_by_chrom_1:\n",
      "    Workflow Options:\n",
      "      --chrom VAL VAL ... (as type, required)\n",
      "  plink_by_chrom_2, plink_by_gene_2:\n",
      "  plink_to_vcf_2:\n",
      "  merge_plink:\n",
      "    Workflow Options:\n",
      "      --name VAL (as str, required)\n",
      "                        File prefix for the analysis output\n",
      "  merge_vcf:\n",
      "    Workflow Options:\n",
      "      --name VAL (as str, required)\n",
      "                        File prefix for the analysis output\n"
     ]
    }
   ],
   "source": [
    "sos run genotype_formatting.ipynb -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "grateful-modern",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[global]\n",
    "# Work directory & output directory\n",
    "parameter: cwd = path(\"output\")\n",
    "# The filename name for containers\n",
    "parameter: container = ''\n",
    "import re\n",
    "parameter: entrypoint= ('micromamba run -a \"\" -n' + ' ' + re.sub(r'(_apptainer:latest|_docker:latest|\\.sif)$', '', container.split('/')[-1])) if container else \"\"\n",
    "# For cluster jobs, number commands to run per job\n",
    "parameter: job_size = 1\n",
    "# Wall clock time expected\n",
    "parameter: walltime = \"5h\"\n",
    "# Memory expected\n",
    "parameter: mem = \"16G\"\n",
    "# Number of threads\n",
    "parameter: numThreads = 20\n",
    "# the path to a bed file or VCF file, a vector of bed files or VCF files, or a text file listing the bed files or VCF files to process\n",
    "parameter: genoFile = paths\n",
    "# use this function to edit memory string for PLINK input\n",
    "from sos.utils import expand_size\n",
    "cwd = f\"{cwd:a}\"\n",
    "\n",
    "import os\n",
    "def get_genotype_file(geno_file_paths):\n",
    "    #\n",
    "    def valid_geno_file(x):\n",
    "        suffixes = path(x).suffixes\n",
    "        if suffixes[-1] == '.bed':\n",
    "            return True\n",
    "        if len(suffixes)>1 and ''.join(suffixes[-2:]) == \".vcf.gz\":\n",
    "            return True\n",
    "        return False\n",
    "    #\n",
    "    def complete_geno_path(x, geno_file):\n",
    "        if not valid_geno_file(x):\n",
    "            raise ValueError(f\"Genotype file {x} should be VCF (end with .vcf.gz) or PLINK bed file (end with .bed)\")\n",
    "        if not os.path.isfile(x):\n",
    "            # relative path\n",
    "            if not os.path.isfile(f'{geno_file:ad}/' + x):\n",
    "                raise ValueError(f\"Cannot find genotype file {x}\")\n",
    "            else:\n",
    "                x = f'{geno_file:ad}/' + x\n",
    "        return x\n",
    "    # \n",
    "    def format_chrom(chrom):\n",
    "        if chrom.startswith('chr'):\n",
    "            chrom = chrom[3:]\n",
    "        return chrom\n",
    "    # Inputs are either VCF or bed, or a vector of them \n",
    "    if len(geno_file_paths) > 1:\n",
    "        if all([valid_geno_file(x) for x in geno_file_paths]):\n",
    "            return paths(geno_file_paths)\n",
    "        else: \n",
    "            raise ValueError(f\"Invalid input {geno_file_paths}\")\n",
    "    # Input is one genotype file or text list of genotype files\n",
    "    geno_file = geno_file_paths[0]\n",
    "    if valid_geno_file(geno_file):\n",
    "        return paths(geno_file)\n",
    "    else: \n",
    "        units = [x.strip().split() for x in open(geno_file).readlines() if x.strip() and not x.strip().startswith('#')]\n",
    "        if all([len(x) == 1 for x in units]):\n",
    "            return paths([complete_geno_path(x[0], geno_file) for x in units])\n",
    "        elif all([len(x) == 2 for x in units]):\n",
    "            genos = dict([(format_chrom(x[0]), path(complete_geno_path(x[1], geno_file))) for x in units])\n",
    "        else:\n",
    "            raise ValueError(f\"{geno_file} should contain one column of file names, or two columns of chrom number and corresponding file name\")\n",
    "        return genos\n",
    "                        \n",
    "genoFile = get_genotype_file(genoFile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enclosed-measure",
   "metadata": {
    "kernel": "SoS",
    "tags": []
   },
   "source": [
    "## PLINK to VCF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "increased-techno",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[plink_to_vcf_1]\n",
    "if isinstance(genoFile, dict):\n",
    "    genoFile = genoFile.values()\n",
    "\n",
    "input: genoFile, group_by = 1\n",
    "output: f'{cwd}/{_input:bn}.vcf.gz'\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime,  mem = mem, cores = numThreads, tags = f'{step_name}_{_output[0]:bn}'\n",
    "bash: expand= \"${ }\", stderr = f'{_output:nn}.stderr', stdout = f'{_output:nn}.stdout', container = container, entrypoint=entrypoint\n",
    "    plink2 --bfile ${_input:n} \\\n",
    "        --recode vcf-iid  \\\n",
    "        --out ${_output:nn} \\\n",
    "        --threads ${numThreads} \\\n",
    "        --memory ${int(expand_size(mem) * 0.9)/1e06} --output-chr chrMT\n",
    "    bgzip -l 9 ${_output:n}\n",
    "    tabix -f -p vcf ${_output}\n",
    "\n",
    "bash: expand= \"${ }\", stderr = f'{_output[0]:n}.stderr', stdout = f'{_output[0]:n}.stdout', container = container, entrypoint=entrypoint\n",
    "    for i in ${_output} ; do \n",
    "        # Capture file metadata\n",
    "        output_info=\"$i\"\n",
    "        output_size=$(ls -lh \"$i\" | awk '{print $5}')\n",
    "        output_rows=$(zcat \"$i\" | wc -l)\n",
    "        output_column=$(zcat \"$i\" | grep -v \"##\" | head -1 | wc -w)\n",
    "        output_header_row=$(zcat \"$i\" | grep \"##\" | wc -l)\n",
    "        output_preview=$(zcat \"$i\" | grep -v \"##\" | head | cut -f 1-11)\n",
    "\n",
    "        # Write captured information to the stdout file\n",
    "        printf \"output_info: %s\\noutput_size: %s\\noutput_rows: %d\\noutput_column: %d\\noutput_header_row: %d\\noutput_preview:\\n%s\\n\" \\\n",
    "            \"$output_info\" \"$output_size\" \"$output_rows\" \"$output_column\" \"$output_header_row\" \"$output_preview\"\n",
    "    done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reverse-charity",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## VCF to PLINK\n",
    "\n",
    "Export VCF files to PLINK 1.0 format, **without keeping allele orders by default**. The resulting PLINK will lose ref/alt allele information but will go by major/minor allele, as conventionally used in standard PLINK format. Notice that PLINK 1.0 format does not allow for dosages. PLINK 2.0 format support it, but it is generally not supported by downstreams data analysis.  \n",
    "\n",
    "In the following code block the option `--vcf-half-call m`  treat half-call as missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "about-characterization",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[vcf_to_plink]\n",
    "parameter: remove_duplicates = False\n",
    "parameter: add_chr = True\n",
    "# The path to the file that contains the list of samples to remove (format FID, IID)\n",
    "parameter: remove_samples = path('.')\n",
    "# The path to the file that contains the list of samples to keep (format FID, IID)\n",
    "parameter: keep_samples = path('.')\n",
    "fail_if(not (keep_samples.is_file() or keep_samples == path('.')), msg = f'Cannot find ``{keep_samples}``')\n",
    "fail_if(not (remove_samples.is_file() or remove_samples == path('.')), msg = f'Cannot find ``{remove_samples}``')\n",
    "\n",
    "if isinstance(genoFile, dict):\n",
    "    genoFile = genoFile.values()\n",
    "\n",
    "input: genoFile, group_by = 1\n",
    "output: f'{cwd}/{_input:bnn}.bed'\n",
    "\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime, mem = mem, cores = numThreads, tags = f'{step_name}_{_output:bn}'\n",
    "bash: container = container, expand= \"${ }\", stderr = f'{_output:n}.stderr', stdout = f'{_output:n}.stdout', entrypoint=entrypoint\n",
    "    plink2 --vcf ${_input} \\\n",
    "        --vcf-half-call m \\\n",
    "        --vcf-require-gt \\\n",
    "        --allow-extra-chr \\\n",
    "        ${('--keep %s' % keep_samples) if keep_samples.is_file() else \"\"} \\\n",
    "        ${('--remove %s' % remove_samples) if remove_samples.is_file() else \"\"} \\\n",
    "        --make-bed --out ${_output:n}  ${\"--rm-dup exclude-all\" if remove_duplicates else \"\" } \\\n",
    "        --threads ${numThreads} \\\n",
    "        --memory ${int(expand_size(mem) * 0.9)/1e06} ${\"--output-chr chrMT\" if add_chr else \"\"}\n",
    "\n",
    "bash: expand= \"${ }\", stderr = f'{_output:n}.stderr', stdout = f'{_output:n}.stdout', container = container, entrypoint=entrypoint\n",
    "        stdout=${_output:n}.stdout\n",
    "        for i in ${_output} ; do \n",
    "            echo \"output_info: $i \" >> $stdout;\n",
    "            echo \"output_size: $(ls -lh \"$i\" | awk '{print $5}')\" >> $stdout;\n",
    "        done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "contained-legislation",
   "metadata": {
    "kernel": "SoS",
    "tags": []
   },
   "source": [
    "## Split PLINK genotypes into specified regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enhanced-definition",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[genotype_by_region_1]\n",
    "# cis window size\n",
    "parameter: window = 0\n",
    "# Region definition\n",
    "parameter: region_list = path\n",
    "regions = [x.strip().split() for x in open(region_list).readlines() if x.strip() and not x.strip().startswith('#')]\n",
    "input: genoFile, for_each = 'regions'\n",
    "output: f'{cwd}/{region_list:bn}_genotype_by_region/{_input:bn}.{_regions[3]}.bed'\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime, mem = mem, cores = numThreads, tags = f'{step_name}_{_output:bn}'\n",
    "bash: expand= \"${ }\", stderr = f'{_output:n}.stderr', stdout = f'{_output:n}.stdout', container = container, entrypoint=entrypoint\n",
    "    plink2 --bfile ${_input:an} \\\n",
    "        --make-bed \\\n",
    "        --out ${_output[0]:n} \\\n",
    "        --chr ${_regions[0]} \\\n",
    "        --from-bp ${f'0' if (int(_regions[1]) - window) < 0 else f'{(int(_regions[1]) - window)}'} \\\n",
    "        --to-bp ${int(_regions[2]) + window} \\\n",
    "        --allow-no-sex --output-chr chrMT || touch ${_output} \n",
    "\n",
    "bash: expand= \"${ }\", stderr = f'{_output:n}.stderr', stdout = f'{_output:n}.stdout', container = container, entrypoint=entrypoint\n",
    "        for i in ${_output} ; do \n",
    "            echo \"output_info: $i \"\n",
    "            echo \"output_size: $(ls -lh \"$i\" | awk '{print $5}')\"\n",
    "        done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prepared-cabinet",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Compute LD matrices for given input region\n",
    "\n",
    "### PLINK based implementation\n",
    "\n",
    "**FIXME: Hao, I suggest including all contents for LD matrix storage type benchmarking into this repo, so we justify why we would like to save the data as square 0, float 16 and using npz format**. Perhaps we should start a folder called \"code/auxillary\" to keep notebooks such as these? You can then remove what you have in the `brain-xqtl-analysis` repository after you migrate all the relevant contents here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "motivated-relaxation",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[ld_by_region_plink_1]\n",
    "# Region definition\n",
    "parameter: region_list = path\n",
    "parameter: float_type = 16\n",
    "regions = [x.strip().split() for x in open(region_list).readlines() if x.strip() and not x.strip().startswith('#')]\n",
    "input: genoFile, for_each = 'regions'\n",
    "output: f'{cwd}/{region_list:bn}_LD/{_input:bn}.{_regions[0]}_{_regions[1]}_{_regions[2]}.float{float_type}.npz'\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime, mem = mem, cores = numThreads, tags = f'{step_name}_{_output:bn}'\n",
    "bash: expand = \"${ }\", stderr = f'{_output:n}.stderr', stdout = f'{_output:n}.stdout', container = container, volumes = [f'{region_list:ad}:{region_list:ad}'], entrypoint=entrypoint\n",
    "    plink --bfile ${_input:an} \\\n",
    "        --out ${_output:nn} \\\n",
    "        --chr ${_regions[0]} \\\n",
    "        --from-bp ${_regions[1]} \\\n",
    "        --to-bp ${_regions[2]} \\\n",
    "        --r square0 \\\n",
    "        --make-just-bim \\\n",
    "        --threads ${numThreads}\n",
    "\n",
    "python: expand= \"${ }\", stderr = f'{_output:n}.stderr', stdout = f'{_output:n}.stdout', container = container, entrypoint=entrypoint\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    np_ld = np.loadtxt(\"${_output:nn}.ld\", delimiter = \"\\t\", dtype = \"float${float_type}\")\n",
    "    bim = pd.read_csv(\"${_output:nn}.bim\", \"\\t\", header = None)[1].to_numpy()\n",
    "    np.savez_compressed(\"${_output}\", np_ld, bim, allow_pickel = True)\n",
    "\n",
    "bash: expand= \"$[ ]\", stderr = f'{_output:n}.stderr', stdout = f'{_output:n}.stdout', container = container, entrypoint=entrypoint\n",
    "        echo \"The npz file is a numpy compressed version of the .ld file described below\"\n",
    "        for i in $[_output:nn] ; do \n",
    "        echo \"output_info: $i.ld \"\n",
    "        echo \"output_size:\" `ls -lh $i.ld | cut -f 5  -d  \" \"`\n",
    "        echo \"output_column:\" `head -1 $i | wc -w `\n",
    "        echo \"output_row:\" `wc -l $i `\n",
    "        done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "generous-television",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "### ldstore2 based implementation\n",
    "\n",
    "This is good for larger sample sizes such as data from UK Biobank although we are not facing this challenge in the FunGen-xQTL project.\n",
    "\n",
    "**FIXME: we need to build ldstore2 into a container image**. According to Diana it should be \n",
    "\n",
    "```\n",
    "pip3 install https://files.pythonhosted.org/packages/a8/fd/f98ab7dea176f42cb61b80450b795ef19b329e8eb715b87b0d13c2a0854d/ldstore-0.1.9.tar.gz \n",
    "```\n",
    "\n",
    "**FIXME: Diana, what's the input for this workflow?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "empirical-remedy",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "#### Create `master` file for `ldstore2`\n",
    "\n",
    "The master file is a semicolon-separated text file and contains no space. It contains the following mandatory column names and one dataset per line:\n",
    "\n",
    "**FIXME: Diana, this documentation is not clearly written. I cannot understand it. What are the mandatory column names? What does it mean by one data-set per line?**\n",
    "\n",
    "- For the Z file, the format should be `rsid:chrom:pos:a1:a2`. Formatting for chromosome should be `01,02,03` etc\n",
    "- List of samples\n",
    "\n",
    "**The LDstore draft is currently availale [here](https://github.com/statgenetics/UKBB_GWAS_dev/blob/master/workflow/111722_LDstore.ipynb) with the code to prepare for the genotypic input [here](https://github.com/statgenetics/UKBB_GWAS_dev/blob/master/workflow/113022_bgenix_ldblocks.ipynb). A minimal working example can be found [here]**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "central-signal",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Split PLINK by Chromosome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "searching-guarantee",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[genotype_by_chrom_1]\n",
    "stop_if(len(paths(genoFile))>1, msg = \"This workflow expects one input genotype file.\")\n",
    "parameter: chrom = list\n",
    "chrom = list(set(chrom))\n",
    "input: genoFile, for_each = \"chrom\"\n",
    "output: f'{cwd}/{_input:bn}.{_chrom}.bed'\n",
    "# look up for genotype file\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime, mem = mem, cores = numThreads, tags = f'{step_name}_{_output:bn}'\n",
    "bash: expand= \"$[ ]\", stderr = f'{_output:n}.stderr', stdout = f'{_output:n}.stdout', container = container, volumes = [f'{genoFile:ad}:{genoFile:ad}'], entrypoint=entrypoint\n",
    "    ##### Get the locus genotypes for $[_chrom]\n",
    "    plink2 --bfile $[_input:an] \\\n",
    "    --make-bed \\\n",
    "    --out $[_output[0]:n] \\\n",
    "    --chr $[_chrom] \\\n",
    "    --threads $[numThreads] \\\n",
    "    --memory $[int(expand_size(mem) * 0.9)/1e06] \\\n",
    "    --allow-no-sex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "careful-sacramento",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[genotype_by_chrom_2]\n",
    "input: group_by = \"all\"\n",
    "output: f'{_input[0]:nn}.{step_name[:-2]}_files.txt'\n",
    "sos_run(\"write_data_list\", data_files = _input, out = _output, ext = \"bed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "experienced-luxury",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[plink_to_vcf_2]\n",
    "input: group_by = \"all\"\n",
    "output: f'{_input[0]:nnn}.{step_name[:-2]}_files.txt'\n",
    "sos_run(\"write_data_list\", data_files = _input, out = _output, ext = \"vcf.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "single-blackjack",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[genotype_by_region_2]\n",
    "input: group_by = \"all\"\n",
    "output: f'{_input[0]:nn}.{step_name[:-2]}_files.txt'\n",
    "sos_run(\"write_data_list\", data_files = _input, out = _output, ext = \"bed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enormous-grocery",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[ld_by_region_*_2]\n",
    "parameter: region_list = path\n",
    "input: group_by = \"all\"\n",
    "output: f'{cwd}/{region_list:bn}_LD/{genoFile:bn}.ld.list'\n",
    "sos_run(\"write_data_list\", data_files = _input, out = _output, ext = \"npy.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fossil-pharmacy",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[write_data_list]\n",
    "parameter: out = path\n",
    "parameter: ext = str\n",
    "parameter: data_files = paths\n",
    "input: data_files\n",
    "output: out\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime, mem = mem, cores = numThreads, tags = f'{step_name}_{_output:bn}'\n",
    "python: expand= \"${ }\", stderr = f'{_output:n}.stderr', stdout = f'{_output:n}.stdout', container = container, entrypoint=entrypoint\n",
    "    import pandas as pd\n",
    "    # Extracting the id list outside for better readability\n",
    "\n",
    "    n = len(\"${ext}\".split(\".\"))+1\n",
    "    id_list = [x.split(\".\")[-n] for x in [${_input:r,}]]\n",
    "\n",
    "    data_tempt = pd.DataFrame({\n",
    "        \"#id\": id_list,\n",
    "        \"#path\": [${_input:r,}]\n",
    "    })\n",
    "\n",
    "    data_tempt.to_csv(\"${_output}\", index=False, sep=\"\\t\")\n",
    "\n",
    "bash: expand= \"${ }\", stderr = f'{_output:n}.stderr', stdout = f'{_output:n}.stdout', container = container, entrypoint=entrypoint\n",
    "    i=\"${_output}\"\n",
    "    output_size=$(ls -lh $i | cut -f 5 -d ' ')\n",
    "    output_rows=$(cat $i | wc -l | cut -f 1 -d ' ')\n",
    "    output_column=$(cat $i | head -1 | wc -w)\n",
    "    output_preview=$(cat $i | grep -v \"##\" | head | cut -f 1,2,3,4,5,6)\n",
    "    \n",
    "    printf \"output_info: %s\\noutput_size: %s\\noutput_rows: %s\\noutput_column: %s\\noutput_preview:\\n%s\\n\" \\\n",
    "        \"$i\" \"$output_size\" \"$output_rows\" \"$output_column\" \"$output_preview\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fifty-quebec",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Split VCF by Chromosome\n",
    "\n",
    "**FIXME: add this as needed**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polished-overall",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Merge PLINK files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "weird-neighbor",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[merge_plink]\n",
    "skip_if(len(genoFile) == 1)\n",
    "# File prefix for the analysis output\n",
    "parameter: name = str\n",
    "# The path to the file that contains the list of samples to keep (format FID, IID)\n",
    "parameter: keep_samples = path('.')\n",
    "input: genoFile, group_by = 'all'\n",
    "output: f\"{cwd}/{name}.bed\"\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime, mem = mem, cores = numThreads, tags = f'{step_name}_{_output:bn}'\n",
    "\n",
    "bash: container=container, expand= \"${ }\", stderr = f'{_output:n}.stderr', stdout = f'{_output:n}.stdout', entrypoint=entrypoint\n",
    "    echo ${_input:n} | tr ' ' '\\n' | tail -n +2 >  ${_output:n}.merge_list\n",
    "    plink2 \\\n",
    "    --bfile ${_input[0]:n} \\\n",
    "    --merge-list ${_output:n}.merge_list \\\n",
    "    --make-bed \\\n",
    "    --out ${_output:n} \\\n",
    "    --threads ${numThreads} \\\n",
    "    --memory ${int(expand_size(mem) * 0.9)/1e06} ${('--keep %s' % keep_samples) if keep_samples.is_file() else \"\"} \n",
    "    rm -f ${_output:n}.merge_list\n",
    "\n",
    "bash: expand= \"${ }\", stderr = f'{_output:n}.stderr', stdout = f'{_output:n}.stdout', container = container, entrypoint=entrypoint\n",
    "    i=\"${_output}\"\n",
    "    output_size=$(ls -lh $i | cut -f 5 -d ' ')\n",
    "    output_rows=$(zcat $i | wc -l | cut -f 1 -d ' ')\n",
    "    output_column=$(zcat $i | head -1 | wc -w)\n",
    "    output_preview=$(cat $i | grep -v \"##\" | head | cut -f 1,2,3,4,5,6)\n",
    "    \n",
    "    printf \"output_info: %s\\noutput_size: %s\\noutput_rows: %s\\noutput_column: %s\\noutput_preview:\\n%s\\n\" \\\n",
    "        \"$i\" \"$output_size\" \"$output_rows\" \"$output_column\" \"$output_preview\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "overall-drink",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Merge VCF files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "variable-metropolitan",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[merge_vcf]\n",
    "skip_if(len(genoFile) == 1)\n",
    "# File prefix for the analysis output\n",
    "parameter: name = str\n",
    "input: genoFile, group_by = 'all'\n",
    "output:  f\"{cwd}/{name}.vcf.gz\"\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime, mem = mem, cores = numThreads, tags = f'{step_name}_{_output:bn}'\n",
    "bash: container=container, expand= \"${ }\", stderr = f'{_output:n}.stderr', stdout = f'{_output:n}.stdout', entrypoint=entrypoint\n",
    "    bcftools concat -Oz ${_input} > ${_output}\n",
    "    tabix -p vcf ${_output}\n",
    "\n",
    "bash: expand= \"${ }\", stderr = f'{_output[0]:n}.stderr', stdout = f'{_output[0]:n}.stdout', container = container, entrypoint=entrypoint\n",
    "    for i in ${_output} ; do \n",
    "        # Capture file metadata\n",
    "        output_info=\"$i\"\n",
    "        output_size=$(ls -lh \"$i\" | awk '{print $5}')\n",
    "        output_rows=$(zcat \"$i\" | wc -l)\n",
    "        output_column=$(zcat \"$i\" | grep -v \"##\" | head -1 | wc -w)\n",
    "        output_header_row=$(zcat \"$i\" | grep \"##\" | wc -l)\n",
    "        output_preview=$(zcat \"$i\" | grep -v \"##\" | head | cut -f 1-11)\n",
    "\n",
    "        # Write captured information to the stdout file\n",
    "        printf \"output_info: %s\\noutput_size: %s\\noutput_rows: %d\\noutput_column: %d\\noutput_header_row: %d\\noutput_preview:\\n%s\\n\" \\\n",
    "            \"$output_info\" \"$output_size\" \"$output_rows\" \"$output_column\" \"$output_header_row\" \"$output_preview\" >> ${_output:n}.stdout\n",
    "    done"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SoS",
   "language": "sos",
   "name": "sos"
  },
  "language_info": {
   "codemirror_mode": "sos",
   "file_extension": ".sos",
   "mimetype": "text/x-sos",
   "name": "sos",
   "nbconvert_exporter": "sos_notebook.converter.SoS_Exporter",
   "pygments_lexer": "sos"
  },
  "sos": {
   "kernels": [
    [
     "Bash",
     "bash",
     "Bash",
     "#E6EEFF",
     ""
    ],
    [
     "SoS",
     "sos",
     "",
     "",
     "sos"
    ]
   ],
   "version": "0.20.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
