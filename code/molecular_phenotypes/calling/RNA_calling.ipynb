{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "vocal-senior",
   "metadata": {
    "kernel": "SoS",
    "tags": []
   },
   "source": [
    "# Quantifying expression from RNA-seq data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blond-compound",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "parental-eugene",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "Our pipeline follows the GTEx pipeline from the [GTEx](https://gtexportal.org/home/aboutGTEx#staticTextAnalysisMethods)/[TOPMed](https://github.com/broadinstitute/gtex-pipeline/blob/master/TOPMed_RNAseq_pipeline.md) project for the quantification of expression from RNA-seq data. Either paired end or single end fastq.gz files may be used as the initial input. Different read [strandedness options](https://rnabio.org/module-09-appendix/0009/12/01/StrandSettings/) are possible, including rf, fr, unstranded or strand_missing, based on library types from Signal et al [[cf. Signal et al (2022)](https://doi.org/10.1186/s12859-022-04572-7)]. Strand detection steps are included in this pipeline to automaticaly detect the strand of the input fastq file by levradging the gene count table ouptut from [STAR](https://physiology.med.cornell.edu/faculty/skrabanek/lab/angsd/lecture_notes/STARmanual.pdf). Read length data is required and is set to a default value of 100 for reads of zero length. \n",
    "\n",
    "We recommend using fastqc to quality control reads, followed by the removal of adaptors with fastp if necessary. After quality control has been conducted, STAR may be used to align reads to the reference genome. We combine this mapping step with an additionaly quality control step using Picard to mark duplicate reads and collect other metrics. \n",
    "\n",
    "Gene-level RNA expression is called with RNA-SeQC v2.4.2 using a reference gtf that has been collapsed to contain genes instead of gene transcripts [[cf. DeLuca et al., Bioinformatics, 2012](https://doi.org/10.1093/bioinformatics/bts196)]. Reads are only included if they are uniquely mapped (mapping quality of 255 for STAR BAMs), if they are aligned in proper pairs, if the read alignment distance was less than or equal to six (i.e. alignments must not contain more than six non-reference bases), and if they are fully contained within exon boundaries. Reads overlapping introns are not included. Exon level read counts are also called by RNA-SeQC. If a read overlaps multiple exons, then a fractional value equal to the portion of the read contained within the exon is allotted. We call transcript-level RNA expression using RSEM v1.3.0 with a transcript reference gtf. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bacterial-bouquet",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Input\n",
    "\n",
    "A tab delimited file containing sample read information with one line per sample. This file should include a header for the following columns:\n",
    "\n",
    "1. `ID` - sample ID (required)\n",
    "2. `fq1` - name of the fastq file (required)\n",
    "3. `fq2` - name of the second fastq file of the sample if using paired end data (required only if using paired end data)\n",
    "4. `strand` - the strandedness of the sample (optional)  \n",
    "[Options include](https://rnabio.org/module-09-appendix/0009/12/01/StrandSettings/) rf, fr, unstranded or strand_missing, based on library types from Signal et al [[cf. Signal et al. 2022](https://doi.org/10.1186/s12859-022-04572-7)]. Strand detection steps are included in this pipeline to automaticaly detect the strand of the input fastq file by leveraging the gene count table ouptut from STAR.  \n",
    "5. `read_length` - the read length of the sample's reads (optional)  \n",
    "If this is unknown, enter 0 and the pipeline will use a default read length of 100. This default can be changed using the `--sjdbOverhang` parameter. If none of the samples have read length information then please leave out the read_length column so that the pipeline uses the default length for each sample. \n",
    "\n",
    "## Output\n",
    "\n",
    "Gene and transcript expression matrices in addition to other intermediate files (see `Command interface` steps below)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "noticed-chambers",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Minimal Working Example Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "robust-masters",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "### i. Perform data quality summary via `fastqc`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "demographic-genre",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "Timing: <4 min"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "supposed-tamil",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "Generate fastqc report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "expanded-burden",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Input samples are paired-end sequences.\n",
      "INFO: Running \u001b[32mfastqc\u001b[0m: \n",
      "INFO: t658723dd3ed068c0 \u001b[32msubmitted\u001b[0m to neurology with job id Your job 1276524 (\"job_t658723dd3ed068c0\") has been submitted\n",
      "INFO: tc92f05117308f947 \u001b[32msubmitted\u001b[0m to neurology with job id Your job 1276525 (\"job_tc92f05117308f947\") has been submitted\n",
      "INFO: t58643d17a6195b4f \u001b[32msubmitted\u001b[0m to neurology with job id Your job 1276526 (\"job_t58643d17a6195b4f\") has been submitted\n",
      "INFO: t1a99ddebeba018cd \u001b[32msubmitted\u001b[0m to neurology with job id Your job 1276527 (\"job_t1a99ddebeba018cd\") has been submitted\n",
      "INFO: t97f7edab7bfa34c9 \u001b[32msubmitted\u001b[0m to neurology with job id Your job 1276528 (\"job_t97f7edab7bfa34c9\") has been submitted\n",
      "INFO: te58c72a940c609c9 \u001b[32msubmitted\u001b[0m to neurology with job id Your job 1276529 (\"job_te58c72a940c609c9\") has been submitted\n",
      "INFO: tbd6bbd7e31a7bf9f \u001b[32msubmitted\u001b[0m to neurology with job id Your job 1276530 (\"job_tbd6bbd7e31a7bf9f\") has been submitted\n",
      "INFO: td7817e7fae153921 \u001b[32msubmitted\u001b[0m to neurology with job id Your job 1276531 (\"job_td7817e7fae153921\") has been submitted\n",
      "INFO: ta00ee58307b5155b \u001b[32msubmitted\u001b[0m to neurology with job id Your job 1276532 (\"job_ta00ee58307b5155b\") has been submitted\n",
      "INFO: tb0cfe2a320a7fae9 \u001b[32msubmitted\u001b[0m to neurology with job id Your job 1276533 (\"job_tb0cfe2a320a7fae9\") has been submitted\n",
      "INFO: t901afcd540b38c65 \u001b[32msubmitted\u001b[0m to neurology with job id Your job 1276534 (\"job_t901afcd540b38c65\") has been submitted\n",
      "INFO: tc401f0ee0e42e4f7 \u001b[32msubmitted\u001b[0m to neurology with job id Your job 1276535 (\"job_tc401f0ee0e42e4f7\") has been submitted\n",
      "INFO: td184ada4d02e14bd \u001b[32msubmitted\u001b[0m to neurology with job id Your job 1276536 (\"job_td184ada4d02e14bd\") has been submitted\n",
      "INFO: t6c450ac7fd351a1e \u001b[32msubmitted\u001b[0m to neurology with job id Your job 1276537 (\"job_t6c450ac7fd351a1e\") has been submitted\n",
      "INFO: t1f9f240218f6bae9 \u001b[32msubmitted\u001b[0m to neurology with job id Your job 1276538 (\"job_t1f9f240218f6bae9\") has been submitted\n",
      "INFO: tdd5f393c7f9514ce \u001b[32msubmitted\u001b[0m to neurology with job id Your job 1276539 (\"job_tdd5f393c7f9514ce\") has been submitted\n",
      "INFO: t45b8fc9f7d538c23 \u001b[32msubmitted\u001b[0m to neurology with job id Your job 1276540 (\"job_t45b8fc9f7d538c23\") has been submitted\n",
      "INFO: t3ff7c8ca7251237f \u001b[32msubmitted\u001b[0m to neurology with job id Your job 1276541 (\"job_t3ff7c8ca7251237f\") has been submitted\n",
      "INFO: ta6509204a71fa9e5 \u001b[32msubmitted\u001b[0m to neurology with job id Your job 1276542 (\"job_ta6509204a71fa9e5\") has been submitted\n",
      "INFO: t8c1d526b7ea29e30 \u001b[32msubmitted\u001b[0m to neurology with job id Your job 1276543 (\"job_t8c1d526b7ea29e30\") has been submitted\n",
      "INFO: Waiting for the completion of \u001b[32m20\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m20\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m20\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m17\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m5\u001b[0m tasks.\n",
      "\u001b[91mERROR\u001b[0m: \u001b[91m[fastqc]: [fastqc]: Output target /restricted/projectnb/casa/frank/xqtl_project/paper/output_test/1000-PCC.bam.1.fq.gz_fastqc.html does not exist after the completion of step fastqc\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!sos run RNA_calling.ipynb fastqc \\\n",
    "    --cwd ../output_test \\\n",
    "    --samples ../PCC_sample_list_subset \\\n",
    "    --data-dir /restricted/projectnb/amp-ad/ROSMAP_PCC_AC/PCC/ \\\n",
    "    --container ../rna_quantification.sif \\\n",
    "    -c ../csg.yml  -q neurology"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "honest-thing",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "### ii. Cut adaptor (Optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confused-exclusive",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "Timing: 8 min"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "secret-mistake",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "Trim adaptors with fastp. A new sample list will be generated witht the trimmed fastq files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cutting-awareness",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "!sos run RNA_calling.ipynb fastqc \\\n",
    "    --cwd ../output_test \\\n",
    "    --samples ../PCC_sample_list_subset \\\n",
    "    --data-dir /restricted/projectnb/amp-ad/ROSMAP_PCC_AC/PCC/ \\\n",
    "    --container ../rna_quantification.sif \\\n",
    "    -c ../csg.yml  -q neurology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bearing-motorcycle",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/restricted/projectnb/casa/frank/xqtl_project/paper/bulk_expression\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "adjusted-indonesian",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Input samples are paired-end sequences.\n",
      "INFO: Running \u001b[32mfastp_trim_adaptor_1\u001b[0m: \n",
      "INFO: t6747fede4c28692d \u001b[32msubmitted\u001b[0m to neurology with job id Your job 1277263 (\"job_t6747fede4c28692d\") has been submitted\n",
      "INFO: taee4e6efc2ac93d6 \u001b[32msubmitted\u001b[0m to neurology with job id Your job 1277265 (\"job_taee4e6efc2ac93d6\") has been submitted\n",
      "INFO: tfb5f75e3305f1651 \u001b[32msubmitted\u001b[0m to neurology with job id Your job 1277267 (\"job_tfb5f75e3305f1651\") has been submitted\n",
      "INFO: t356f37597d962db8 \u001b[32msubmitted\u001b[0m to neurology with job id Your job 1277269 (\"job_t356f37597d962db8\") has been submitted\n",
      "INFO: t0b873616f06ed762 \u001b[32msubmitted\u001b[0m to neurology with job id Your job 1277271 (\"job_t0b873616f06ed762\") has been submitted\n",
      "INFO: t92fe6aaad84f242b \u001b[32msubmitted\u001b[0m to neurology with job id Your job 1277273 (\"job_t92fe6aaad84f242b\") has been submitted\n",
      "INFO: tf691ed7f1f2578df \u001b[32msubmitted\u001b[0m to neurology with job id Your job 1277275 (\"job_tf691ed7f1f2578df\") has been submitted\n",
      "INFO: t19dcdb94fee46da7 \u001b[32msubmitted\u001b[0m to neurology with job id Your job 1277277 (\"job_t19dcdb94fee46da7\") has been submitted\n",
      "INFO: tca07f271e942eda0 \u001b[32msubmitted\u001b[0m to neurology with job id Your job 1277279 (\"job_tca07f271e942eda0\") has been submitted\n",
      "INFO: t7dc83836146bdd84 \u001b[32msubmitted\u001b[0m to neurology with job id Your job 1277281 (\"job_t7dc83836146bdd84\") has been submitted\n",
      "INFO: Waiting for the completion of \u001b[32m10\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m10\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m10\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m9\u001b[0m tasks.\n",
      "INFO: \u001b[32mfastp_trim_adaptor_1\u001b[0m output:   \u001b[32m/restricted/projectnb/casa/frank/xqtl_project/paper/output_test/1000-PCC.bam.1.fq.trimmed.fq.gz /restricted/projectnb/casa/frank/xqtl_project/paper/output_test/1000-PCC.bam.2.fq.trimmed.fq.gz... (20 items in 10 groups)\u001b[0m\n",
      "INFO: Running \u001b[32mfastp_trim_adaptor_2\u001b[0m: \n",
      "INFO: \u001b[32mfastp_trim_adaptor_2\u001b[0m is \u001b[32mcompleted\u001b[0m.\n",
      "INFO: \u001b[32mfastp_trim_adaptor_2\u001b[0m output:   \u001b[32m/restricted/projectnb/casa/frank/xqtl_project/paper/output_test/../PCC_sample_list_subset.trimmed.txt\u001b[0m\n",
      "INFO: Workflow fastp_trim_adaptor (ID=w054e59ef59a63067) is executed successfully with 2 completed steps, 11 completed substeps and 10 completed tasks.\n"
     ]
    }
   ],
   "source": [
    "!sos run RNA_calling.ipynb fastp_trim_adaptor \\\n",
    "    --cwd ../output_test \\\n",
    "    --samples ../PCC_sample_list_subset \\\n",
    "    --data-dir /restricted/projectnb/amp-ad/ROSMAP_PCC_AC/PCC/ \\\n",
    "    --STAR-index ../../../../skandoi/ROSMAP_DLPFC/reference_data/STAR_Index/ \\\n",
    "    --gtf ../../reference_data/reference_data/Homo_sapiens.GRCh38.103.chr.reformatted.ERCC.gtf \\\n",
    "    --container ../rna_quantification.sif \\\n",
    "    --reference-fasta ../../../../skandoi/reference_data/GRCh38_full_analysis_set_plus_decoy_hla.noALT_noHLA_noDecoy_ERCC.fasta \\\n",
    "    --ref-flat ../../../../skandoi/ROSMAP_DLPFC/reference_data/Homo_sapiens.GRCh38.103.chr.reformated.ERCC.gtf.ref.flat \\\n",
    "    -c ../csg.yml  -q neurology"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prescription-vinyl",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "### iii. Read alignment via STAR and QC via Picard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "engaged-interim",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "Timing <2 hours"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "right-setup",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "Align the reads with STAR and generate the bam_list recipe for downstream molecular phenotype count matrixes. This step also checks the standedness of the data and removes duplicates with Picard. The `--varVCFfile` and `--chrom_size_file` options are required to enable the wasp filter in STAR. This is required if aligning reads to generate splicing data later in the pipeline. This is not required if quantifying expression for eQTLs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fifty-socket",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "kernel": "SoS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Input samples are paired-end sequences.\n",
      "INFO: Running \u001b[32mSTAR_align_1\u001b[0m: \n",
      "Insufficent memory for STAR, changing to 40G\n",
      "Using read length specified in the sample list\n",
      "Using read length specified in the sample list\n",
      "Using read length specified in the sample list\n",
      "Using read length specified in the sample list\n",
      "Using read length specified in the sample list\n",
      "Using read length specified in the sample list\n",
      "Using read length specified in the sample list\n",
      "Using read length specified in the sample list\n",
      "Using read length specified in the sample list\n",
      "Using read length specified in the sample list\n",
      "INFO: tba625c3ee8e2ebc6 \u001b[32msubmitted\u001b[0m to neurology with job id Your job 1589961 (\"job_tba625c3ee8e2ebc6\") has been submitted\n",
      "INFO: tabbdef1e00dd9ad5 \u001b[32msubmitted\u001b[0m to neurology with job id Your job 1589962 (\"job_tabbdef1e00dd9ad5\") has been submitted\n",
      "INFO: t9a6cf5553ce43e52 \u001b[32msubmitted\u001b[0m to neurology with job id Your job 1589963 (\"job_t9a6cf5553ce43e52\") has been submitted\n",
      "INFO: t4b17cb2b722c28d5 \u001b[32msubmitted\u001b[0m to neurology with job id Your job 1589964 (\"job_t4b17cb2b722c28d5\") has been submitted\n",
      "INFO: t5dcc6a5b59fa2159 \u001b[32msubmitted\u001b[0m to neurology with job id Your job 1589965 (\"job_t5dcc6a5b59fa2159\") has been submitted\n",
      "INFO: t2318c37130b3d5e8 \u001b[32msubmitted\u001b[0m to neurology with job id Your job 1589966 (\"job_t2318c37130b3d5e8\") has been submitted\n",
      "INFO: t58f7d73b64770fff \u001b[32msubmitted\u001b[0m to neurology with job id Your job 1589967 (\"job_t58f7d73b64770fff\") has been submitted\n",
      "INFO: ta950fbca3f181203 \u001b[32msubmitted\u001b[0m to neurology with job id Your job 1589968 (\"job_ta950fbca3f181203\") has been submitted\n",
      "INFO: tf96a1f2b62c98513 \u001b[32msubmitted\u001b[0m to neurology with job id Your job 1589969 (\"job_tf96a1f2b62c98513\") has been submitted\n",
      "INFO: t0cf5112f35139d2e \u001b[32msubmitted\u001b[0m to neurology with job id Your job 1589970 (\"job_t0cf5112f35139d2e\") has been submitted\n",
      "INFO: Waiting for the completion of \u001b[32m10\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m10\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m10\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m10\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m10\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m10\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m10\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m10\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m10\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m10\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m10\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m10\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m10\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m10\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m10\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m10\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m10\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m10\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m10\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m10\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m10\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m10\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m10\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m9\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m9\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m9\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m7\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m6\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m6\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m6\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m6\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m5\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m3\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m2\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m2\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m2\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m1\u001b[0m task.\n",
      "INFO: Waiting for the completion of \u001b[32m1\u001b[0m task.\n",
      "INFO: Waiting for the completion of \u001b[32m1\u001b[0m task.\n",
      "INFO: Waiting for the completion of \u001b[32m1\u001b[0m task.\n",
      "INFO: Waiting for the completion of \u001b[32m1\u001b[0m task.\n",
      "INFO: \u001b[32mSTAR_align_1\u001b[0m output:   \u001b[32m/restricted/projectnb/casa/frank/xqtl_project/paper/output_test/star_output/1000-PCC.bam.Aligned.sortedByCoord.out.bam /restricted/projectnb/casa/frank/xqtl_project/paper/output_test/star_output/1000-PCC.bam.Aligned.toTranscriptome.out.bam... (20 items in 10 groups)\u001b[0m\n",
      "INFO: Running \u001b[32mstrand_detected_1\u001b[0m: \n",
      "for sample 1000-PCC.bam\n",
      "Counts for the 2nd read strand aligned with RNA is 0.9945125919850982, > 90% of aligned count\n",
      "Data is likely RF/fr-firststrand\n",
      "INFO: \u001b[32mstrand_detected_1\u001b[0m (index=0) is \u001b[32mcompleted\u001b[0m.\n",
      "for sample 1001-PCC.bam\n",
      "Counts for the 2nd read strand aligned with RNA is 0.9946500588593552, > 90% of aligned count\n",
      "Data is likely RF/fr-firststrand\n",
      "INFO: \u001b[32mstrand_detected_1\u001b[0m (index=1) is \u001b[32mcompleted\u001b[0m.\n",
      "for sample 1002-PCC.bam\n",
      "Counts for the 2nd read strand aligned with RNA is 0.9943981411103944, > 90% of aligned count\n",
      "Data is likely RF/fr-firststrand\n",
      "INFO: \u001b[32mstrand_detected_1\u001b[0m (index=2) is \u001b[32mcompleted\u001b[0m.\n",
      "for sample 1003-PCC.bam\n",
      "Counts for the 2nd read strand aligned with RNA is 0.99371513806055, > 90% of aligned count\n",
      "Data is likely RF/fr-firststrand\n",
      "INFO: \u001b[32mstrand_detected_1\u001b[0m (index=3) is \u001b[32mcompleted\u001b[0m.\n",
      "for sample 1004-PCC.bam\n",
      "Counts for the 2nd read strand aligned with RNA is 0.9947175340513231, > 90% of aligned count\n",
      "Data is likely RF/fr-firststrand\n",
      "INFO: \u001b[32mstrand_detected_1\u001b[0m (index=4) is \u001b[32mcompleted\u001b[0m.\n",
      "for sample 1005-PCC.bam\n",
      "Counts for the 2nd read strand aligned with RNA is 0.9943227514543671, > 90% of aligned count\n",
      "Data is likely RF/fr-firststrand\n",
      "INFO: \u001b[32mstrand_detected_1\u001b[0m (index=5) is \u001b[32mcompleted\u001b[0m.\n",
      "for sample 1006-PCC.bam\n",
      "Counts for the 2nd read strand aligned with RNA is 0.9951195009722332, > 90% of aligned count\n",
      "Data is likely RF/fr-firststrand\n",
      "INFO: \u001b[32mstrand_detected_1\u001b[0m (index=6) is \u001b[32mcompleted\u001b[0m.\n",
      "for sample 1009-PCC.bam\n",
      "Counts for the 2nd read strand aligned with RNA is 0.9941384572915384, > 90% of aligned count\n",
      "Data is likely RF/fr-firststrand\n",
      "INFO: \u001b[32mstrand_detected_1\u001b[0m (index=7) is \u001b[32mcompleted\u001b[0m.\n",
      "for sample 1010-PCC.bam\n",
      "Counts for the 2nd read strand aligned with RNA is 0.9905496531374057, > 90% of aligned count\n",
      "Data is likely RF/fr-firststrand\n",
      "INFO: \u001b[32mstrand_detected_1\u001b[0m (index=8) is \u001b[32mcompleted\u001b[0m.\n",
      "for sample 1011-PCC.bam\n",
      "Counts for the 2nd read strand aligned with RNA is 0.994062350839579, > 90% of aligned count\n",
      "Data is likely RF/fr-firststrand\n",
      "INFO: \u001b[32mstrand_detected_1\u001b[0m (index=9) is \u001b[32mcompleted\u001b[0m.\n",
      "INFO: Running \u001b[32mstrand_detected_2\u001b[0m: \n",
      "Using strand specified in the input samples list ['rf', 'rf', 'rf', 'rf', 'rf', 'rf', 'rf', 'rf', 'rf', 'rf'], replacing strand_missing with detected strand\n",
      "INFO: \u001b[32mstrand_detected_2\u001b[0m is \u001b[32mcompleted\u001b[0m.\n",
      "INFO: Running \u001b[32mSTAR_align_2\u001b[0m: \n",
      "INFO: ta6bc63475f33d6d3 \u001b[32msubmitted\u001b[0m to neurology with job id Your job 1590384 (\"job_ta6bc63475f33d6d3\") has been submitted\n",
      "INFO: t5f69283dbd9f3268 \u001b[32msubmitted\u001b[0m to neurology with job id Your job 1590385 (\"job_t5f69283dbd9f3268\") has been submitted\n",
      "INFO: t2a9b2ea2cd548304 \u001b[32msubmitted\u001b[0m to neurology with job id Your job 1590386 (\"job_t2a9b2ea2cd548304\") has been submitted\n",
      "INFO: t40fbd36264adc5e8 \u001b[32msubmitted\u001b[0m to neurology with job id Your job 1590387 (\"job_t40fbd36264adc5e8\") has been submitted\n",
      "INFO: tb56c2e19492598e9 \u001b[32msubmitted\u001b[0m to neurology with job id Your job 1590388 (\"job_tb56c2e19492598e9\") has been submitted\n",
      "INFO: t603b04f096a1a2f8 \u001b[32msubmitted\u001b[0m to neurology with job id Your job 1590389 (\"job_t603b04f096a1a2f8\") has been submitted\n",
      "INFO: t073e59cd55b8cce7 \u001b[32msubmitted\u001b[0m to neurology with job id Your job 1590390 (\"job_t073e59cd55b8cce7\") has been submitted\n",
      "INFO: t9338476b73fdeb01 \u001b[32msubmitted\u001b[0m to neurology with job id Your job 1590391 (\"job_t9338476b73fdeb01\") has been submitted\n",
      "INFO: t4cd542d154ba8707 \u001b[32msubmitted\u001b[0m to neurology with job id Your job 1590392 (\"job_t4cd542d154ba8707\") has been submitted\n",
      "INFO: t2a9bf84472649a4c \u001b[32msubmitted\u001b[0m to neurology with job id Your job 1590393 (\"job_t2a9bf84472649a4c\") has been submitted\n",
      "INFO: Waiting for the completion of \u001b[32m10\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m10\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m10\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m10\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m10\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m10\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m10\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m10\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m10\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m10\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m10\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m10\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m10\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m10\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m10\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m10\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m10\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m10\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m10\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m10\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m10\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m10\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m10\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m10\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m10\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m10\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m10\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m10\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m10\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m10\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m10\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m10\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m10\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m10\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m10\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m10\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m10\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m10\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m9\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m9\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m9\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m9\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m8\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m8\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m7\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m7\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m7\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m7\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m6\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m5\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m5\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m5\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m5\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m4\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m3\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m3\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m3\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m2\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m2\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m2\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m2\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m1\u001b[0m task.\n",
      "INFO: Waiting for the completion of \u001b[32m1\u001b[0m task.\n",
      "INFO: Waiting for the completion of \u001b[32m1\u001b[0m task.\n",
      "INFO: Waiting for the completion of \u001b[32m1\u001b[0m task.\n",
      "INFO: \u001b[32mSTAR_align_2\u001b[0m output:   \u001b[32m/restricted/projectnb/casa/frank/xqtl_project/paper/output_test/star_output/1000-PCC.bam.alignment_summary_metrics /restricted/projectnb/casa/frank/xqtl_project/paper/output_test/star_output/1000-PCC.bam.rna_metrics... (40 items in 10 groups)\u001b[0m\n",
      "INFO: Running \u001b[32mSTAR_align_3\u001b[0m: \n",
      "INFO: tc05b61420c52d464 \u001b[32msubmitted\u001b[0m to neurology with job id Your job 1590563 (\"job_tc05b61420c52d464\") has been submitted\n",
      "INFO: Waiting for the completion of \u001b[32m1\u001b[0m task.\n",
      "INFO: Waiting for the completion of \u001b[32m1\u001b[0m task.\n",
      "INFO: Waiting for the completion of \u001b[32m1\u001b[0m task.\n",
      "INFO: Waiting for the completion of \u001b[32m1\u001b[0m task.\n",
      "INFO: Waiting for the completion of \u001b[32m1\u001b[0m task.\n",
      "INFO: Waiting for the completion of \u001b[32m1\u001b[0m task.\n",
      "INFO: Waiting for the completion of \u001b[32m1\u001b[0m task.\n",
      "INFO: \u001b[32mSTAR_align_3\u001b[0m output:   \u001b[32m/restricted/projectnb/casa/frank/xqtl_project/paper/output_test/star_output/PCC_sample_list_subset_bam_list\u001b[0m\n",
      "INFO: Workflow STAR_align (ID=w387ca43d9e82e72f) is executed successfully with 5 completed steps, 32 completed substeps and 21 completed tasks.\n"
     ]
    }
   ],
   "source": [
    "#no STAR wasp filter:\n",
    "!sos run RNA_calling.ipynb STAR_align \\\n",
    "    --cwd ../output_test/star_output \\\n",
    "    --samples ../PCC_sample_list_subset \\\n",
    "    --data-dir /restricted/projectnb/amp-ad/ROSMAP_PCC_AC/PCC/ \\\n",
    "    --STAR-index ../../../../skandoi/ROSMAP_DLPFC/reference_data/STAR_Index/ \\\n",
    "    --gtf ../../reference_data/Homo_sapiens.GRCh38.103.chr.reformatted.ERCC.gtf \\\n",
    "    --container ../rna_quantification.sif \\\n",
    "    --reference-fasta ../../../../skandoi/reference_data/GRCh38_full_analysis_set_plus_decoy_hla.noALT_noHLA_noDecoy_ERCC.fasta \\\n",
    "    --ref-flat ../../../../skandoi/ROSMAP_DLPFC/reference_data/Homo_sapiens.GRCh38.103.chr.reformated.ERCC.gtf.ref.flat \\\n",
    "    -s build  -c ../csg.yml  -q neurology "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forty-princess",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "#include STAR wasp filter:\n",
    "!sos run RNA_calling.ipynb STAR_align \\\n",
    "    --cwd ../output_test \\\n",
    "    --samples ../PCC_sample_list_subset \\\n",
    "    --data-dir /restricted/projectnb/amp-ad/ROSMAP_PCC_AC/PCC/ \\\n",
    "    --STAR-index ../../../../skandoi/ROSMAP_DLPFC/reference_data/STAR_Index/ \\\n",
    "    --gtf ../../reference_data/Homo_sapiens.GRCh38.103.chr.reformatted.ERCC.gtf \\\n",
    "    --container ../rna_quantification.sif \\\n",
    "    --reference-fasta ../../../../skandoi/reference_data/GRCh38_full_analysis_set_plus_decoy_hla.noALT_noHLA_noDecoy_ERCC.fasta \\\n",
    "    --varVCFfile ../../reference_data/ZOD14598_AD_GRM_WGS_2021-04-29_all.recalibrated_variants.leftnorm.filtered.AF.WASP.vcf \\\n",
    "    --ref-flat ../../../../skandoi/ROSMAP_DLPFC/reference_data/Homo_sapiens.GRCh38.103.chr.reformated.ERCC.gtf.ref.flat \\\n",
    "    -s build  -c ../csg.yml  -q neurology "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "soviet-paris",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "### iv. Call gene-level RNA expression via rnaseqc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "damaged-native",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "Timing <30 min"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "skilled-portfolio",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "Call gene-level RNA expression using rnaseqc and run multiqc. The gtf file must include gene-level data instead of transcript-level data. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "brilliant-symposium",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Input samples are paired-end sequences.\n",
      "INFO: Running \u001b[32mrnaseqc_call_1\u001b[0m: \n",
      "INFO: tff3abbfce568e3a6 \u001b[32mre-execute completed\u001b[0m\n",
      "INFO: tdc6ecb02cf3dc9db \u001b[32mre-execute completed\u001b[0m\n",
      "INFO: tff3abbfce568e3a6 \u001b[32msubmitted\u001b[0m to neurology with job id Your job 1604380 (\"job_tff3abbfce568e3a6\") has been submitted\n",
      "INFO: tae3d9f610313974a \u001b[32mre-execute completed\u001b[0m\n",
      "INFO: ta12f432172f27c2f \u001b[32mre-execute completed\u001b[0m\n",
      "INFO: tdc6ecb02cf3dc9db \u001b[32msubmitted\u001b[0m to neurology with job id Your job 1604381 (\"job_tdc6ecb02cf3dc9db\") has been submitted\n",
      "INFO: t285594d60f4606a2 \u001b[32mre-execute completed\u001b[0m\n",
      "INFO: tae3d9f610313974a \u001b[32msubmitted\u001b[0m to neurology with job id Your job 1604382 (\"job_tae3d9f610313974a\") has been submitted\n",
      "INFO: t5aa744bde972a668 \u001b[32mre-execute completed\u001b[0m\n",
      "INFO: ta12f432172f27c2f \u001b[32msubmitted\u001b[0m to neurology with job id Your job 1604383 (\"job_ta12f432172f27c2f\") has been submitted\n",
      "INFO: tdd39e49437af9945 \u001b[32mre-execute completed\u001b[0m\n",
      "INFO: t285594d60f4606a2 \u001b[32msubmitted\u001b[0m to neurology with job id Your job 1604384 (\"job_t285594d60f4606a2\") has been submitted\n",
      "INFO: t60061471403c3279 \u001b[32mre-execute completed\u001b[0m\n",
      "INFO: t5aa744bde972a668 \u001b[32msubmitted\u001b[0m to neurology with job id Your job 1604385 (\"job_t5aa744bde972a668\") has been submitted\n",
      "INFO: t699893e56f190658 \u001b[32mre-execute completed\u001b[0m\n",
      "INFO: tdd39e49437af9945 \u001b[32msubmitted\u001b[0m to neurology with job id Your job 1604386 (\"job_tdd39e49437af9945\") has been submitted\n",
      "INFO: t3f7a45ea7a984362 \u001b[32mre-execute completed\u001b[0m\n",
      "INFO: t60061471403c3279 \u001b[32msubmitted\u001b[0m to neurology with job id Your job 1604387 (\"job_t60061471403c3279\") has been submitted\n",
      "INFO: t699893e56f190658 \u001b[32msubmitted\u001b[0m to neurology with job id Your job 1604388 (\"job_t699893e56f190658\") has been submitted\n",
      "INFO: t3f7a45ea7a984362 \u001b[32msubmitted\u001b[0m to neurology with job id Your job 1604389 (\"job_t3f7a45ea7a984362\") has been submitted\n",
      "INFO: Waiting for the completion of \u001b[32m10\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m10\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m10\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m10\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m10\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m10\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m10\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m10\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m10\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m5\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m2\u001b[0m tasks.\n",
      "INFO: \u001b[32mrnaseqc_call_1\u001b[0m output:   \u001b[32m/restricted/projectnb/casa/frank/xqtl_project/paper/output_test/star_output/1000-PCC.bam.rnaseqc.gene_tpm.gct.gz /restricted/projectnb/casa/frank/xqtl_project/paper/output_test/star_output/1000-PCC.bam.rnaseqc.gene_reads.gct.gz... (40 items in 10 groups)\u001b[0m\n",
      "INFO: Running \u001b[32mrnaseqc_call_2\u001b[0m: \n",
      "INFO: t0ba125532f80a7be \u001b[32mre-execute completed\u001b[0m\n",
      "INFO: t0ba125532f80a7be \u001b[32msubmitted\u001b[0m to neurology with job id Your job 1604413 (\"job_t0ba125532f80a7be\") has been submitted\n",
      "INFO: Waiting for the completion of \u001b[32m1\u001b[0m task.\n",
      "INFO: Waiting for the completion of \u001b[32m1\u001b[0m task.\n",
      "INFO: Waiting for the completion of \u001b[32m1\u001b[0m task.\n",
      "INFO: \u001b[32mrnaseqc_call_2\u001b[0m output:   \u001b[32m/restricted/projectnb/casa/frank/xqtl_project/paper/output_test/star_output/PCC_sample_list_subset.rnaseqc.gene_tpm.gct.gz /restricted/projectnb/casa/frank/xqtl_project/paper/output_test/star_output/PCC_sample_list_subset.rnaseqc.gene_readsCount.gct.gz... (4 items)\u001b[0m\n",
      "INFO: Running \u001b[32mrnaseqc_call_3\u001b[0m: \n",
      "INFO: Step \u001b[32mrnaseqc_call_3\u001b[0m (index=0) is \u001b[32mignored\u001b[0m with signature constructed\n",
      "INFO: Step \u001b[32mrnaseqc_call_3\u001b[0m (index=0) is \u001b[32mignored\u001b[0m with signature constructed\n",
      "INFO: \u001b[32mrnaseqc_call_3\u001b[0m output:   \u001b[32m/restricted/projectnb/casa/frank/xqtl_project/paper/output_test/star_output/PCC_sample_list_subset.multiqc_report.html\u001b[0m\n",
      "INFO: Running \u001b[32mrnaseqc_call_4\u001b[0m: \n",
      "INFO: t33ed0a6d12ec5c40 \u001b[32mrestart\u001b[0m from status \u001b[32mfailed\u001b[0m\n",
      "INFO: t33ed0a6d12ec5c40 \u001b[32msubmitted\u001b[0m to neurology with job id Your job 1604427 (\"job_t33ed0a6d12ec5c40\") has been submitted\n",
      "INFO: Waiting for the completion of \u001b[32m1\u001b[0m task.\n",
      "INFO: Waiting for the completion of \u001b[32m1\u001b[0m task.\n",
      "INFO: Waiting for the completion of \u001b[32m1\u001b[0m task.\n",
      "INFO: Waiting for the completion of \u001b[32m1\u001b[0m task.\n",
      "INFO: Waiting for the completion of \u001b[32m1\u001b[0m task.\n",
      "\u001b[91mERROR\u001b[0m: \u001b[91m[rnaseqc_call_4]: [t33ed0a6d12ec5c40]: Executing script in Singularity returns an error (exitcode=1, stderr=/restricted/projectnb/casa/frank/xqtl_project/paper/output_test/star_output/PCC_sample_list_subset.picard.aggregated_quality.metrics.stderr).\n",
      "The script has been saved to /usr3/graduate/fgrennjr/.sos/57403a2cfeaf03fb//usr3/graduate/fgrennjr/.sos/57403a2cfeaf03fb.To reproduce the error please run:\n",
      "\u001b[0m\u001b[32msingularity exec  ../rna_quantification.sif micromamba run -a \"\" -n rna_quantification Rscript /usr3/graduate/fgrennjr/.sos/57403a2cfeaf03fb/singularity_run_743023.R\u001b[0m\u001b[91m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!sos run RNA_calling.ipynb rnaseqc_call \\\n",
    "    --cwd ../output_test/star_output \\\n",
    "    --samples ../PCC_sample_list_subset \\\n",
    "    --data-dir /restricted/projectnb/amp-ad/ROSMAP_PCC_AC/PCC/ \\\n",
    "    --STAR-index ../../../../skandoi/ROSMAP_DLPFC/reference_data/STAR_Index/ \\\n",
    "    --gtf ../../reference_data/Homo_sapiens.GRCh38.103.chr.reformatted.collapse_only.gene.ERCC.gtf \\\n",
    "    --container ../rna_quantification.sif \\\n",
    "    --reference-fasta ../../../../skandoi/reference_data/GRCh38_full_analysis_set_plus_decoy_hla.noALT_noHLA_noDecoy_ERCC.fasta \\\n",
    "    --ref-flat ../../../../skandoi/ROSMAP_DLPFC/reference_data/Homo_sapiens.GRCh38.103.chr.reformated.ERCC.gtf.ref.flat \\\n",
    "    --bam_list ../output_test/star_output/PCC_sample_list_subset_bam_list \\\n",
    "    -s build  -c ../csg.yml  -q neurology "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acting-intent",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "### v. Call transcript level RNA expression via RSEM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proud-latvia",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "Timing <X hours"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "early-richmond",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "Call transcript level RNA expression using RSEM and run multiqc. The gtf file used as input should match the one used to generate the RSEM index and therefore contain transcript-level, not gene-level, information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "homeless-probe",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Input samples are paired-end sequences.\n",
      "INFO: Running \u001b[32mrsem_call_1\u001b[0m: \n",
      "INFO: Step \u001b[32mrsem_call_1\u001b[0m (index=0) is \u001b[32mignored\u001b[0m with signature constructed\n",
      "INFO: Step \u001b[32mrsem_call_1\u001b[0m (index=1) is \u001b[32mignored\u001b[0m with signature constructed\n",
      "INFO: Step \u001b[32mrsem_call_1\u001b[0m (index=2) is \u001b[32mignored\u001b[0m with signature constructed\n",
      "INFO: Step \u001b[32mrsem_call_1\u001b[0m (index=3) is \u001b[32mignored\u001b[0m with signature constructed\n",
      "INFO: Step \u001b[32mrsem_call_1\u001b[0m (index=4) is \u001b[32mignored\u001b[0m with signature constructed\n",
      "INFO: Step \u001b[32mrsem_call_1\u001b[0m (index=5) is \u001b[32mignored\u001b[0m with signature constructed\n",
      "INFO: Step \u001b[32mrsem_call_1\u001b[0m (index=6) is \u001b[32mignored\u001b[0m with signature constructed\n",
      "INFO: Step \u001b[32mrsem_call_1\u001b[0m (index=7) is \u001b[32mignored\u001b[0m with signature constructed\n",
      "INFO: Step \u001b[32mrsem_call_1\u001b[0m (index=8) is \u001b[32mignored\u001b[0m with signature constructed\n",
      "INFO: Step \u001b[32mrsem_call_1\u001b[0m (index=9) is \u001b[32mignored\u001b[0m with signature constructed\n",
      "INFO: \u001b[32mrsem_call_1\u001b[0m output:   \u001b[32m/restricted/projectnb/casa/frank/xqtl_project/paper/output_test/star_output/1000-PCC.bam.rsem.isoforms.results /restricted/projectnb/casa/frank/xqtl_project/paper/output_test/star_output/1000-PCC.bam.rsem.genes.results... (30 items in 10 groups)\u001b[0m\n",
      "INFO: Running \u001b[32mrsem_call_2\u001b[0m: \n",
      "INFO: t86a6697ca95d7563 \u001b[32msubmitted\u001b[0m to neurology with job id Your job 1604685 (\"job_t86a6697ca95d7563\") has been submitted\n",
      "INFO: Waiting for the completion of \u001b[32m1\u001b[0m task.\n",
      "INFO: Waiting for the completion of \u001b[32m1\u001b[0m task.\n",
      "INFO: Waiting for the completion of \u001b[32m1\u001b[0m task.\n",
      "\u001b[91mERROR\u001b[0m: \u001b[91m[rsem_call_2]: [t86a6697ca95d7563]: Executing script in Singularity returns an error (exitcode=1, stderr=/restricted/projectnb/casa/frank/xqtl_project/paper/output_test/star_output/PCC_sample_list_subset.rsem.aggregated_quality.metrics.stderr).\n",
      "The script has been saved to /usr3/graduate/fgrennjr/.sos/57403a2cfeaf03fb//usr3/graduate/fgrennjr/.sos/57403a2cfeaf03fb.To reproduce the error please run:\n",
      "\u001b[0m\u001b[32msingularity exec  ../rna_quantification.sif micromamba run -a \"\" -n rna_quantification Rscript /usr3/graduate/fgrennjr/.sos/57403a2cfeaf03fb/singularity_run_1499600.R\u001b[0m\u001b[91m\n",
      "[rsem_call]: Exits with 2 pending steps (rsem_call_3, rsem_call_4)\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!sos run RNA_calling.ipynb rsem_call \\\n",
    "    --cwd ../output_test/star_output \\\n",
    "    --samples ../PCC_sample_list_subset \\\n",
    "    --data-dir /restricted/projectnb/amp-ad/ROSMAP_PCC_AC/PCC/ \\\n",
    "    --STAR-index ../../../../skandoi/ROSMAP_DLPFC/reference_data/STAR_Index/ \\\n",
    "    --gtf ../../reference_data/Homo_sapiens.GRCh38.103.chr.reformatted.ERCC.gtf \\\n",
    "    --container ../rna_quantification.sif \\\n",
    "    --reference-fasta ../../../../skandoi/reference_data/GRCh38_full_analysis_set_plus_decoy_hla.noALT_noHLA_noDecoy_ERCC.fasta \\\n",
    "    --ref-flat ../../../../skandoi/ROSMAP_DLPFC/reference_data/Homo_sapiens.GRCh38.103.chr.reformated.ERCC.gtf.ref.flat \\\n",
    "    --bam_list ../output_test/star_output/PCC_sample_list_subset_bam_list \\\n",
    "    --RSEM-index ../../../../skandoi/reference_data/RSEM_Index \\\n",
    "    -s build  -c ../csg.yml  -q neurology "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "perceived-enforcement",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Troubleshooting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stretch-adaptation",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "| Step | Substep | Problem | Possible Reason | Solution |\n",
    "|------|---------|---------|------------------|---------|\n",
    "|  |  |  |  |  |\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "great-expert",
   "metadata": {
    "kernel": "Bash"
   },
   "source": [
    "## Command interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "irish-fever",
   "metadata": {
    "kernel": "Bash"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: sos run RNA_calling.ipynb [workflow_name | -t targets] [options] [workflow_options]\n",
      "  workflow_name:        Single or combined workflows defined in this script\n",
      "  targets:              One or more targets to generate\n",
      "  options:              Single-hyphen sos parameters (see \"sos run -h\" for details)\n",
      "  workflow_options:     Double-hyphen workflow-specific parameters\n",
      "\n",
      "Workflows:\n",
      "  bam_to_fastq\n",
      "  fastqc\n",
      "  fastp_trim_adaptor\n",
      "  trimmomatic_trim_adaptor\n",
      "  STAR_align\n",
      "  strand_detected\n",
      "  picard_qc\n",
      "  rnaseqc_call\n",
      "  rsem_call\n",
      "\n",
      "Global Workflow Options:\n",
      "  --cwd output (as path)\n",
      "                        The output directory for generated files.\n",
      "  --samples VAL (as path, required)\n",
      "                        Sample meta data list\n",
      "  --data-dir  path(f\"{samples:d}\")\n",
      "\n",
      "                        Raw data directory, default to the same directory as\n",
      "                        sample list\n",
      "  --job-size 1 (as int)\n",
      "                        For cluster jobs, number commands to run per job\n",
      "  --walltime 5h\n",
      "                        Wall clock time expected\n",
      "  --mem 16G\n",
      "                        Memory expected\n",
      "  --java-mem 6G\n",
      "                        Memory for Java virtual mechine (`picard`)\n",
      "  --numThreads 8 (as int)\n",
      "                        Number of threads\n",
      "  --container ''\n",
      "                        Software container option\n",
      "  --entrypoint {('micromamba run -a \"\" -n' + ' ' + container.split('/')[-1][:-4]) if container.endswith('.sif') else f''}\n",
      "\n",
      "  --[no-]uncompressed (default to False)\n",
      "                        Whether the fasta/fastq file is compressed or not.\n",
      "\n",
      "Sections\n",
      "  bam_to_fastq:\n",
      "  fastqc:\n",
      "  fastp_trim_adaptor_1:\n",
      "    Workflow Options:\n",
      "      --window-size 4 (as int)\n",
      "                        sliding window setting\n",
      "      --required-quality 20 (as int)\n",
      "      --leading 20 (as int)\n",
      "                        the mean quality requirement option for cut_front\n",
      "      --trailing 20 (as int)\n",
      "                        the mean quality requirement option for cut_tail\n",
      "      --min-len 15 (as int)\n",
      "                        reads shorter than length_required will be discarded\n",
      "      --fasta-with-adapters-etc . (as path)\n",
      "                        Path to the reference adaptors\n",
      "  fastp_trim_adaptor_2:\n",
      "  trimmomatic_trim_adaptor:\n",
      "    Workflow Options:\n",
      "      --fasta-with-adapters-etc . (as path)\n",
      "                        Illumina clip setting Path to the reference adaptors\n",
      "      --seed-mismatches 2 (as int)\n",
      "      --palindrome-clip-threshold 30 (as int)\n",
      "      --simple-clip-threshold 10 (as int)\n",
      "      --window-size 4 (as int)\n",
      "                        sliding window setting\n",
      "      --required-quality 20 (as int)\n",
      "      --leading 3 (as int)\n",
      "                        Other settings\n",
      "      --trailing 3 (as int)\n",
      "      --min-len 50 (as int)\n",
      "  STAR_align_1:\n",
      "    Workflow Options:\n",
      "      --gtf VAL (as path, required)\n",
      "                        Reference gene model\n",
      "      --STAR-index VAL (as path, required)\n",
      "                        STAR indexing file\n",
      "      --outFilterMultimapNmax 20 (as int)\n",
      "      --alignSJoverhangMin 8 (as int)\n",
      "      --alignSJDBoverhangMin 1 (as int)\n",
      "      --outFilterMismatchNmax 999 (as int)\n",
      "      --outFilterMismatchNoverLmax 0.1 (as float)\n",
      "      --alignIntronMin 20 (as int)\n",
      "      --alignIntronMax 1000000 (as int)\n",
      "      --alignMatesGapMax 1000000 (as int)\n",
      "      --outFilterType BySJout\n",
      "      --outFilterScoreMinOverLread 0.33 (as float)\n",
      "      --outFilterMatchNminOverLread 0.33 (as float)\n",
      "      --limitSjdbInsertNsj 1200000 (as int)\n",
      "      --outSAMstrandField intronMotif\n",
      "      --outFilterIntronMotifs None\n",
      "      --alignSoftClipAtReferenceEnds Yes\n",
      "      --quantMode TranscriptomeSAM GeneCounts (as list)\n",
      "      --outSAMattrRGline ID:rg1 SM:sm1 (as list)\n",
      "      --outSAMattributes NH HI AS nM NM ch (as list)\n",
      "      --chimSegmentMin 15 (as int)\n",
      "      --chimJunctionOverhangMin 15 (as int)\n",
      "      --chimOutType Junctions WithinBAM SoftClip (as list)\n",
      "      --chimMainSegmentMultNmax 1 (as int)\n",
      "      --sjdbOverhang 100 (as int)\n",
      "      --varVCFfile . (as path)\n",
      "  strand_detected_1:\n",
      "  strand_detected_2:\n",
      "    Workflow Options:\n",
      "      --strand ''\n",
      "  picard_qc, STAR_align_2:\n",
      "    Workflow Options:\n",
      "      --gtf VAL (as path, required)\n",
      "                        Reference gene model\n",
      "      --ref-flat VAL (as path, required)\n",
      "                        Path to flat reference file, for computing QC metric\n",
      "      --reference-fasta VAL (as path, required)\n",
      "                        The fasta reference file used to generate star index\n",
      "      --optical-distance 100 (as int)\n",
      "                        For the patterned flowcell models (HiSeq X), change to\n",
      "                        2500\n",
      "      --varVCFfile . (as path)\n",
      "  STAR_align_3:\n",
      "  rnaseqc_call_1:\n",
      "    Workflow Options:\n",
      "      --bam-list VAL (as path, required)\n",
      "      --gtf VAL (as path, required)\n",
      "                        Reference gene model\n",
      "      --detection-threshold 5 (as int)\n",
      "      --mapping-quality 255 (as int)\n",
      "  rnaseqc_call_2:\n",
      "  rsem_call_1:\n",
      "    Workflow Options:\n",
      "      --RSEM-index VAL (as path, required)\n",
      "      --max-frag-len 1000 (as int)\n",
      "      --bam-list VAL (as path, required)\n",
      "  rsem_call_2:\n",
      "  rsem_call_3, rnaseqc_call_3:\n",
      "  rsem_call_4, rnaseqc_call_4:\n"
     ]
    }
   ],
   "source": [
    "sos run RNA_calling.ipynb -h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "integrated-hungary",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Setup and global parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aggressive-relaxation",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[global]\n",
    "# The output directory for generated files.\n",
    "parameter: cwd = path(\"output\")\n",
    "# Sample meta data list\n",
    "parameter: samples = path\n",
    "# Raw data directory, default to the same directory as sample list\n",
    "parameter: data_dir = path(f\"{samples:d}\")\n",
    "# For cluster jobs, number commands to run per job\n",
    "parameter: job_size = 1\n",
    "# Wall clock time expected\n",
    "parameter: walltime = \"5h\"\n",
    "# Memory expected\n",
    "parameter: mem = \"16G\"\n",
    "# Memory for Java virtual mechine (`picard`)\n",
    "parameter: java_mem = \"6G\"\n",
    "# Number of threads\n",
    "parameter: numThreads = 8\n",
    "# Software container option\n",
    "parameter: container = \"\"\n",
    "import re\n",
    "parameter: entrypoint= ('micromamba run -a \"\" -n' + ' ' + re.sub(r'(_apptainer:latest|_docker:latest|\\.sif)$', '', container.split('/')[-1])) if container else \"\"\n",
    "from sos.utils import expand_size\n",
    "cwd = path(f'{cwd:a}')\n",
    "## Whether the fasta/fastq file is compressed or not.\n",
    "parameter: uncompressed = False\n",
    "import os\n",
    "import pandas as pd\n",
    "## FIX: The way to get sample needs to be revamped to 1. Accomodate rf/fr as column 2. accomodate single end read (Only 2 fq/samples)\n",
    "sample_inv = pd.read_csv(samples,sep = \"\\t\")\n",
    "## Extract strand information if user have specified the strand\n",
    "strand_inv = []\n",
    "\n",
    "if \"strand\" in sample_inv.columns:\n",
    "    strand_inv = sample_inv.strand.values.tolist()\n",
    "    sample_inv = sample_inv.drop(\"strand\" , axis = 1)\n",
    "    stop_if(not all([x in [\"fr\", \"rf\", \"unstranded\",\"strand_missing\"] for x in strand_inv ]), msg = \"strand columns should only include ``fr``, ``rf``, ``strand_missing`` or ``unstranded``\")\n",
    "            \n",
    "## Extract read_length if user have specified read_length\n",
    "read_length = [0] * len(sample_inv.index)\n",
    "if \"read_length\" in sample_inv.columns:\n",
    "    read_length = sample_inv.read_length.values.tolist()\n",
    "    sample_inv = sample_inv.drop(\"read_length\" , axis = 1)\n",
    "    \n",
    "    \n",
    "## Extract sample_id\n",
    "sample_inv_list = sample_inv.values.tolist()\n",
    "sample_id = [x[0] for x in sample_inv_list]\n",
    "\n",
    "\n",
    "    \n",
    "## Get the file name for single/paired end data\n",
    "file_inv = [x[1:] for x in sample_inv_list]\n",
    "file_inv = [item for sublist in file_inv for item in sublist]\n",
    "\n",
    "raw_reads = [f'{data_dir}/{x}' for x in file_inv]\n",
    "\n",
    "\n",
    "for y in raw_reads:\n",
    "        if not os.path.isfile(y):\n",
    "            raise ValueError(f\"File {y} does not exist\")\n",
    "\n",
    "if len(raw_reads) != len(set(raw_reads)):\n",
    "        raise ValueError(\"Duplicated files are found (but should not be allowed) in sample file list\")\n",
    "\n",
    "# Is the RNA-seq data pair-end\n",
    "is_paired_end = 0 if len(raw_reads) == len(sample_id) else 1 \n",
    "from sos.utils import env\n",
    "env.logger.info(f'Input samples are {\"paired-end\" if is_paired_end else \"single-end\"} sequences.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "violent-skill",
   "metadata": {
    "kernel": "SoS",
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Step 0: Convert BAM back to fastq if necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sized-national",
   "metadata": {
    "kernel": "SoS",
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "[bam_to_fastq]\n",
    "input: raw_reads, group_by = 1\n",
    "output: f'{cwd}/{_input:bn}.1.fastq',f'{cwd}/{_input:bn}.2.fastq'\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime, mem = mem, cores = numThreads\n",
    "bash: expand= \"${ }\", stderr = f'{_output[0]:n}.stderr', stdout = f'{_output[0]:n}.stdout', container=container, entrypoint=entrypoint\n",
    "    samtools sort -n ${_input} -o ${_output[0]:nn}.sorted.bam\n",
    "    bedtools bamtofastq -i ${_output[0]:nn}.sorted.bam -fq ${_output[0]} -fq2 ${_output[1]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "appointed-invitation",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Step 1: QC before alignment\n",
    "\n",
    "This step utilize `fastqc` and will generate two QC report in `html` format. It should be noted that the [paired_end reads will also be qc separately](https://www.biostars.org/p/190584/)\n",
    "\n",
    "### Step Inputs\n",
    "\n",
    "* `fastq1` and `fastq2`: paths to original `fastq.gz` file.\n",
    "\n",
    "### Step Outputs\n",
    "* Two `html` file for QC report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "choice-vienna",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[fastqc]\n",
    "input: raw_reads, group_by =  1\n",
    "output: f'{cwd}/{_input:bn}_fastqc.html',f'{cwd}/{_input:bn}_fastqc.zip' \n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime, mem = mem, cores = numThreads\n",
    "bash: expand= \"${ }\", stderr = f'{_output[0]:n}.stderr', stdout = f'{_output[0]:n}.stdout', container=container, entrypoint=entrypoint\n",
    "    fastqc ${_input} -o ${_output[0]:d}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "patent-partition",
   "metadata": {
    "kernel": "SoS",
    "tags": []
   },
   "source": [
    "## Step 2: Remove adaptor through `fastp`\n",
    "\n",
    "Documentation: [fastp](https://github.com/OpenGene/fastp)\n",
    "\n",
    "We use `fastp` in place of the `Trimmomatic` for fastp's ability to detect adapter from the reads. It was a c++ command line tool published in [Sept 2018](https://academic.oup.com/bioinformatics/article/34/17/i884/5093234). It  will use the following algorithm to detect the adaptors:\n",
    ">The adapter-sequence detection algorithm is based on two assumptions: the first is that only one adapter exists in the data; the second is that adapter sequences exist only in the read tails. These two assumptions are valid for major next-generation sequencers like Illumina HiSeq series, NextSeq series and NovaSeq series. We compute the k-mer (k = 10) of first N reads (N = 1 M). From this k-mer, the sequences with high occurrence frequencies (>0.0001) are considered as adapter seeds. Low-complexity sequences are removed because they are usually caused by sequencing artifacts. The adapter seeds are sorted by its occurrence frequencies. A tree-based algorithm is applied to extend the adapter seeds to find the real complete adapter\n",
    "\n",
    "It was demostrated that fastp can remove all the adaptor automatically and completely faster than Trimmomatic and cutadapt\n",
    "\n",
    "### Step Inputs\n",
    "\n",
    "* `fastq`: 1 set of fq.gz file for each sample. (2 files if paired end, 1 file if single end )\n",
    "\n",
    "### Step Outputs\n",
    "* 1 set of  `fastq.gz` file for alignment. (2 files if paired end, 1 file if single end )\n",
    "* The unpaired reads (i.e.where a read survived, but the partner read did not.) will be discarded by default as those were not used in the following steps. This feature can be added were it was needed. \n",
    "* 1 set of html documenting the quality of input reads(1 html file and 1 json file)\n",
    "\n",
    "### Step options\n",
    "\n",
    "A few options were selected to be customizable so that fastp step can have the same level of flexiblity as the Trimmomatic step had. They are:\n",
    "- min_len: length_required, reads shorter than this will be discarded, default is 15. (int [=15])\n",
    "- window_size: cut_window_size, the window size option shared by cut_front, cut_tail or cut_sliding. Range: 1~1000, default: 4 (int [=4])\n",
    "- leading/trailing : cut_front_mean_quality/cut_tail_mean_quality, the mean quality requirement option for cut_front/cut_tail, which move a sliding window from front/tail, drop the bases in the window if its mean quality < threshold. **Notice the choice of quality score in `fastp` (N=20) is a lot higher than that of `trimmomatic` (N=3)**. Default `fastp` setting is in line with that of [`cutadapt`](https://cutadapt.readthedocs.io/en/stable/guide.html).\n",
    "\n",
    "The default value are set to be the same as the default value of the `fastp` software."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "japanese-leather",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[fastp_trim_adaptor_1]\n",
    "# sliding window setting\n",
    "parameter: window_size = 4\n",
    "parameter: required_quality = 20\n",
    "# the mean quality requirement option for cut_front\n",
    "parameter: leading = 20\n",
    "# the mean quality requirement option for cut_tail\n",
    "parameter: trailing = 20\n",
    "# reads shorter than length_required will be discarded\n",
    "parameter: min_len = 15\n",
    "# Path to the reference adaptors\n",
    "parameter: fasta_with_adapters_etc = path(\".\")\n",
    "warn_if(fasta_with_adapters_etc.is_file(),msg = \"Use input fasta and adaptor detection of paired-end read was disabled\" )\n",
    "\n",
    "input: raw_reads, group_by = is_paired_end + 1 , group_with = \"sample_id\"\n",
    "output: [f'{cwd}/{path(x):bn}.trimmed.fq.gz' for x in _input]\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime, mem = mem, cores = numThreads\n",
    "bash:  container=container,expand= \"${ }\", stderr = f'{_output[0]:n}.stderr', stdout = f'{_output[0]:n}.stdout', entrypoint=entrypoint\n",
    "        fastp -i ${f'{_input[0]} -I {_input[1]}' if is_paired_end else _input} -o ${ f'{_output[0]} -O {_output[1]}' if is_paired_end else _output } \\\n",
    "            ${f'--adapter_fasta {fasta_with_adapters_etc}' if fasta_with_adapters_etc.is_file() else \"--detect_adapter_for_pe\"}  -V -h ${_output[0]:n}.html -j ${_output[0]:n}.json -w ${numThreads} \\\n",
    "            --length_required ${min_len}  -W ${window_size} -M ${required_quality} -5 -3 --cut_front_mean_quality ${leading} --cut_tail_mean_quality ${leading}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incoming-baptist",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[fastp_trim_adaptor_2]\n",
    "input: group_by = \"all\"\n",
    "output: f'{cwd}/{samples:n}.trimmed.txt'\n",
    "sample_inv_tmp = pd.read_csv(samples,sep = \"\\t\")\n",
    "import csv\n",
    "if is_paired_end:\n",
    "    sample_inv_tmp.fq1 = [f'{x:r}'.replace(\"'\",\"\") for x in _input][::2]\n",
    "    sample_inv_tmp.fq2 = [f'{x:r}'.replace(\"'\",\"\") for x in _input][1::2]\n",
    "else:\n",
    "    sample_inv_tmp.fq1 = [f'{x:r}'.replace(\"'\",\"\") for x in _input]\n",
    "sample_inv_tmp.to_csv(_output,sep = \"\\t\",index = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "later-block",
   "metadata": {
    "kernel": "SoS",
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "## Step 2 Alternative: Remove adaptor through `Trimmomatic`\n",
    "\n",
    "Documentation: [Trimmomatic](http://www.usadellab.org/cms/?page=trimmomatic)\n",
    "\n",
    "We have replaced this with `fastp`, see above, which performs better than `Trimmomatic` in terms of removing adaptors that `Trimmomatic` cannot detect. `fastp` can also automatically guess the adapter sequences from data and by default no adapter sequence is required for input to `fastp`.\n",
    "\n",
    "### Step Inputs\n",
    "\n",
    "* `software_dir`: directory for the software\n",
    "* `fasta_with_adapters_etc`: **filename** for the adapter reference file. According to `Trimmomatic` documention,\n",
    "\n",
    "> As a rule of thumb newer libraries will use `TruSeq3`, but this really depends on your service provider. If you use FASTQC, the \"Overrepresented Sequences\" report can help indicate which adapter file is best suited for your data. \"Illumina Single End\" or \"Illumina Paired End\" sequences indicate single-end or paired-end `TruSeq2` libraries, and the appropriate adapter files are `TruSeq2-SE.fa` and `TruSeq2-PE.fa` respectively. \"TruSeq Universal Adapter\" or \"TruSeq Adapter, Index …\" indicates `TruSeq-3` libraries, and the appropriate adapter files are `TruSeq3-SE.fa` or `TruSeq3-PE.fa`, for single-end and paired-end data respectively. Adapter sequences for `TruSeq2` multiplexed libraries, indicated by \"Illumina Multiplexing \n",
    "…\", and the various RNA library preparations are not currently included.\n",
    "\n",
    "We have `fastqc` workflow previously defined and executed. Users should decide what fasta adapter reference to use based on `fastqc` results (or their own knowledge).\n",
    "\n",
    "### Step Outputs\n",
    "* Two paired `fastq.gz` file for alignment\n",
    "* Two unpaired `fastq.gz` \n",
    "\n",
    ">For single-ended data, one input and one output file are specified, plus the processing steps. For paired-end data, two input files are specified, and 4 output files, 2 for the 'paired' output where both reads survived the processing, and 2 for corresponding 'unpaired' output where a read survived, but the partner read did not.\n",
    "\n",
    "\n",
    "**You need to figure out from fastqc results what adapter reference sequence to use.**, eg `--fasta_with_adapters_etc TruSeq3-PE.fa`. These files can be downloaded from `Trimmomatic` github repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "worldwide-personality",
   "metadata": {
    "kernel": "SoS",
    "tags": []
   },
   "outputs": [],
   "source": [
    "[trimmomatic_trim_adaptor]\n",
    "# Illumina clip setting\n",
    "# Path to the reference adaptors\n",
    "parameter: fasta_with_adapters_etc = path(\".\")\n",
    "parameter: seed_mismatches = 2\n",
    "parameter: palindrome_clip_threshold = 30\n",
    "parameter: simple_clip_threshold = 10\n",
    "# sliding window setting\n",
    "parameter: window_size = 4\n",
    "parameter: required_quality = 20\n",
    "# Other settings\n",
    "parameter: leading = 3\n",
    "parameter: trailing = 3\n",
    "parameter: min_len = 50\n",
    "input: raw_reads, group_by = is_paired_end + 1 , group_with = \"sample_id\"\n",
    "output: ([ f'{cwd}/{_sample_id}_paired_{_input[0]:bn}.gz', f'{cwd}/{_sample_id}_unpaired_{_input[0]:bn}.gz',  f'{cwd}/{_sample_id}_paired_{_input[1]:bn}.gz',f'{cwd}/{_sample_id}_unpaired_{_input[1]:bn}.gz' ] if is_paired_end else f'{cwd}/{_sample_id}_trimmed_{_input:bn}.gz')\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime, mem = mem, cores = numThreads\n",
    "bash: container=container, expand= \"${ }\", stderr = f'{_output[0]:n}.stderr', stdout = f'{_output[0]:n}.stdout', entrypoint=entrypoint\n",
    "    trimmomatic -Xmx${java_mem} ${\"PE\" if is_paired_end else \"SE\"}  -threads ${numThreads} \\\n",
    "        ${_input:r}  ${_output:r} \\\n",
    "        ILLUMINACLIP:${fasta_with_adapters_etc}:${seed_mismatches}:${palindrome_clip_threshold}:${simple_clip_threshold} \\\n",
    "        LEADING:${leading} TRAILING:${trailing} SLIDINGWINDOW:${window_size}:${required_quality} MINLEN:${min_len}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dense-village",
   "metadata": {
    "kernel": "SoS",
    "toc-hr-collapsed": true
   },
   "source": [
    "## Step 3: Alignment through `STAR`\n",
    "\n",
    "Documentation : [STAR](https://github.com/alexdobin/STAR) and [Script in docker](https://github.com/broadinstitute/gtex-pipeline/blob/master/rnaseq/src/run_STAR.py)\n",
    "\n",
    "This step is the main step for `STAR` alignment. \n",
    "\n",
    "Originally, the STAR alignment is implemented using the run_STAR.py command from gtex. However, in order to [accomodate different read length among samples](https://github.com/cumc/xqtl-pipeline/issues/318), We reimplement what was included in the wrapper, including the re-alignment of STAR unsorted bam using samtools to avoid high mem consumption.  \n",
    "\n",
    "\n",
    "\n",
    "### Step Inputs\n",
    "\n",
    "* paths to trimmed `fastq.gz` file from Step 1 as documented in the new sample list.\n",
    "* `STAR_index`: directory for the STAR aligment index\n",
    "\n",
    "### Step Outputs\n",
    "\n",
    "* bam file output `${cwd}/{sample_id}.Aligned.sortedByCoord.bam`, will be used in step 3 and 4\n",
    "* bam file output `${cwd}/{sample_id}.Aligned.toTranscriptome.bam`, will be used in step 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "large-steal",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[STAR_align_1]\n",
    "# Reference gene model\n",
    "parameter: gtf = path\n",
    "# STAR indexing file\n",
    "parameter: STAR_index = path\n",
    "parameter: outFilterMultimapNmax = 20 \n",
    "parameter: alignSJoverhangMin = 8 \n",
    "parameter: alignSJDBoverhangMin = 1 \n",
    "parameter: outFilterMismatchNmax = 999 \n",
    "parameter: outFilterMismatchNoverLmax = 0.1 #larger than Yang's group (outFilterMismatchNoverReadLmax 0.04)\n",
    "parameter: alignIntronMin = 20 \n",
    "parameter: alignIntronMax = 1000000 \n",
    "parameter: alignMatesGapMax = 1000000 \n",
    "parameter: outFilterType =  \"BySJout\" \n",
    "parameter: outFilterScoreMinOverLread = 0.33 \n",
    "parameter: outFilterMatchNminOverLread = 0.33 \n",
    "parameter: limitSjdbInsertNsj = 1200000 \n",
    "parameter: outSAMstrandField = \"intronMotif\" \n",
    "parameter: outFilterIntronMotifs = \"None\" \n",
    "parameter: alignSoftClipAtReferenceEnds = \"Yes\" \n",
    "parameter: quantMode = [\"TranscriptomeSAM\", \"GeneCounts\"]\n",
    "parameter: outSAMattrRGline = [\"ID:rg1\", \"SM:sm1\"]\n",
    "parameter: outSAMattributes = [\"NH\", \"HI\", \"AS\", \"nM\", \"NM\", \"ch\"] #(Yang's group: outSAMattributes NH HI AS nM XS vW). vW added is varVCFfile is set\n",
    "parameter: chimSegmentMin = 15 \n",
    "parameter: chimJunctionOverhangMin = 15 \n",
    "parameter: chimOutType = [\"Junctions\", \"WithinBAM\", \"SoftClip\"]\n",
    "parameter: chimMainSegmentMultNmax = 1 \n",
    "parameter: sjdbOverhang = 100\n",
    "parameter: varVCFfile = path(\".\")\n",
    "if int(mem.replace(\"G\",\"\")) <  40:\n",
    "    print(\"Insufficent memory for STAR, changing to 40G\")\n",
    "    star_mem = '40G'\n",
    "else:\n",
    "    star_mem = mem\n",
    "# This option is commented out because it will force the downstream analysis to use 40G, which significantlly slow down the process.\n",
    "input: raw_reads,group_by = is_paired_end + 1,group_with = {\"sample_id\",\"read_length\"}\n",
    "output: cord_bam = f'{cwd}/{_sample_id}.Aligned.sortedByCoord.out{\"_wasp_qc\" if varVCFfile.is_file() else \"\"}.bam',\n",
    "        trans_bam = f'{cwd}/{_sample_id}.Aligned.toTranscriptome.out{\"_wasp_qc\" if varVCFfile.is_file() else \"\"}.bam'\n",
    "if _read_length == 0:\n",
    "    print(\"Using specified --sjdbOverhang as read length\")\n",
    "else:\n",
    "    print(\"Using read length specified in the sample list\")\n",
    "if varVCFfile.is_file():\n",
    "    print(\"Adding wasp filter in STAR alignment\")\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime, mem = star_mem, cores = numThreads\n",
    "bash: container=container, expand= \"$[ ]\", stderr = f'{_output[0]:n}.stderr', stdout = f'{_output[0]:n}.stdout', entrypoint=entrypoint\n",
    "    rm -rf $[cwd]/$[_sample_id].*.out.*.gz\n",
    "    rm -rf $[cwd]/$[_sample_id]._STARpass1\n",
    "    set -e\n",
    "    touch $[_output[0]:n].star_start.timestamp\n",
    "    STAR --runMode alignReads \\\n",
    "        --runThreadN $[numThreads] \\\n",
    "        --genomeDir  $[STAR_index] \\\n",
    "        --readFilesIn  $[_input:r] \\\n",
    "        --readFilesCommand $[\"cat\" if uncompressed else \"zcat\"] \\\n",
    "        --outFileNamePrefix $[_output[0]:nnnn]. \\\n",
    "        --outSAMstrandField $[outSAMstrandField] \\\n",
    "        --twopassMode Basic \\\n",
    "        --outFilterMultimapNmax $[outFilterMultimapNmax] \\\n",
    "        --alignSJoverhangMin $[alignSJoverhangMin] \\\n",
    "        --alignSJDBoverhangMin $[alignSJDBoverhangMin] \\\n",
    "        --outFilterMismatchNmax $[outFilterMismatchNmax] \\\n",
    "        --outFilterMismatchNoverLmax $[outFilterMismatchNoverLmax] \\\n",
    "        --alignIntronMin $[alignIntronMin] \\\n",
    "        --alignIntronMax $[alignIntronMax] \\\n",
    "        --alignMatesGapMax $[alignMatesGapMax] \\\n",
    "        --outFilterType $[outFilterType] \\\n",
    "        --outFilterScoreMinOverLread $[outFilterScoreMinOverLread] \\\n",
    "        --outFilterMatchNminOverLread $[outFilterMatchNminOverLread] \\\n",
    "        --limitSjdbInsertNsj $[limitSjdbInsertNsj] \\\n",
    "        --outFilterIntronMotifs $[outFilterIntronMotifs] \\\n",
    "        --alignSoftClipAtReferenceEnds $[alignSoftClipAtReferenceEnds] \\\n",
    "        --quantMode $[\" \".join(quantMode)] \\\n",
    "        --outSAMtype BAM Unsorted \\\n",
    "        --outSAMunmapped Within \\\n",
    "        --genomeLoad NoSharedMemory \\\n",
    "        --chimSegmentMin $[chimSegmentMin] \\\n",
    "        --chimJunctionOverhangMin $[chimJunctionOverhangMin] \\\n",
    "        --chimOutType $[\" \".join(chimOutType)] \\\n",
    "        --chimMainSegmentMultNmax $[chimMainSegmentMultNmax] \\\n",
    "        --chimOutJunctionFormat 0 \\\n",
    "        --outSAMattributes $[\" \".join(outSAMattributes)] $[\"vW\" if varVCFfile.is_file() else \"\"] \\\n",
    "        --outSAMattrRGline $[\" \".join(outSAMattrRGline)] \\\n",
    "        --sjdbOverhang $[sjdbOverhang if _read_length == 0 else _read_length  ] \\\n",
    "        --sjdbGTFfile $[gtf] $[(\"--varVCFfile %s --waspOutputMode SAMtag\" % varVCFfile) if varVCFfile.is_file() else \"\"]\n",
    "\n",
    "    rm -r $[_output[0]:nnnn]._STARgenome\n",
    "    $[ f'rm -rf [_output[0]:nnnn]._STARtmp' if is_paired_end else \"\"]\n",
    "    touch $[_output[0]:n].sort_start.timestamp\n",
    "    # According to GTEX, this can help reducing the amount of memory consumption.\n",
    "    samtools sort --threads $[numThreads] -o $[_output[0]:nnnn].Aligned.sortedByCoord.out.bam $[_output[0]:nnnn].Aligned.out.bam \n",
    "    rm $[_output[0]:nnnn].Aligned.out.bam \n",
    "\n",
    "    # WASP based QC\n",
    "    wasp_filter() {\n",
    "    # Check if an argument is provided\n",
    "    if [[ -z \"$1\" ]]; then\n",
    "        echo \"Usage: wasp_filter <file.something.ext>\"\n",
    "        return 1\n",
    "    fi\n",
    "\n",
    "    local input_file=\"$1\"\n",
    "    \n",
    "    # Derive file.ext from file.something.ext\n",
    "    local base_name=\"${input_file%.*}\"  # Removes the last extension\n",
    "    local derived_file=\"${base_name%.*}.out.bam\"  # Removes the second to last extension and adds .ext\n",
    "    \n",
    "    # Execute the command\n",
    "    samtools view -h -q 255 \"$derived_file\" | grep -v \"vW:i:[2-7]\" | samtools view -b > \"$input_file\"\n",
    "    \n",
    "    # Delete the derived_file after using it, to save storage, if you don't want do that, comment out\n",
    "    rm \"$derived_file\"\n",
    "    }\n",
    "\n",
    "    export -f wasp_filter  # To make it available in subshells, if needed.\n",
    "\n",
    "    # Yang's group did't add `-q 255`, but GTEx group add `-q 255` here\n",
    "    $[ f'wasp_filter  {_output[0]}' if varVCFfile.is_file() else \"\"]\n",
    "    $[ f'wasp_filter  {_output[1]}' if varVCFfile.is_file() else \"\"]\n",
    "    \n",
    "\n",
    "    samtools index $[_output[0]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "derived-philosophy",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[strand_detected_1: shared=\"step_strand_detected\"]\n",
    "input: output_from(\"STAR_align_1\")[\"trans_bam\"], group_with = \"sample_id\"\n",
    "import pandas as pd\n",
    "ReadsPerGene = pd.read_csv(f'{cwd}/{_sample_id}.ReadsPerGene.out.tab' , sep = \"\\t\" , header = None )\n",
    "ReadsPerGene_list = ReadsPerGene.loc[3::,].sum(axis = 0, numeric_only = True).values.tolist()\n",
    "strand_percentage = [x/ReadsPerGene_list[0] for x in ReadsPerGene_list]\n",
    "print(f'for sample {_sample_id}')\n",
    "if strand_percentage[1] > 0.9:\n",
    "    print(f'Counts for the 1st read strand aligned with RNA is {strand_percentage[1]}, > 90% of aligned count')\n",
    "    print('Data is likely FR/fr-secondstrand')\n",
    "    strand_detected = \"fr\"\n",
    "elif  strand_percentage[2] > 0.9:\n",
    "    print(f'Counts for the 2nd read strand aligned with RNA is {strand_percentage[2]}, > 90% of aligned count')\n",
    "    print('Data is likely RF/fr-firststrand')\n",
    "    strand_detected = \"rf\"\n",
    "elif max( strand_percentage[1],  strand_percentage[2]) < 0.6:\n",
    "    print(f'Both {strand_percentage[1]} and  {strand_percentage[2]} are under 60% of reads explained by one direction')\n",
    "    print('Data is likely unstranded')\n",
    "    strand_detected = \"unstranded\"\n",
    "else:\n",
    "    strand_detected = 'strand_missing' \n",
    "    print(f'Data does not fall into a likely stranded (max percent explained {max( strand_percentage[1],  strand_percentage[2])} > 0.9) or unstranded layout (max percent explained {max( strand_percentage[1],  strand_percentage[2])} < 0.6), please check your data and manually specified the ``--strand`` option as ``fr``, ``rf`` or ``unstranded``')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "norwegian-maria",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[strand_detected_2: shared=\"strand\"]\n",
    "input: group_by = \"all\"\n",
    "parameter: strand = \"\"\n",
    "import pandas as pd\n",
    "if not strand:\n",
    "    if len(strand_inv) > 0:\n",
    "        strand = strand_inv\n",
    "        for i in range(0,len(strand)):\n",
    "            if strand[i] == \"strand_missing\":\n",
    "                strand[i] = step_strand_detected[i]\n",
    "        print(f'Using strand specified in the input samples list {strand}, replacing strand_missing with detected strand')\n",
    "    else:\n",
    "        warn_if(not all(x is step_strand_detected[0] for x in step_strand_detected), msg = \"strands detected are different among samples, please check your protocol, we will use the detected strand for each samples\")    \n",
    "        strand = step_strand_detected\n",
    "        print(f'Using detected strand for each samples {strand}')\n",
    "else:\n",
    "    stop_if(strand not in [\"fr\", \"rf\", \"unstranded\"], msg = \"``--strand`` option should be ``fr``, ``rf`` or ``unstranded``\")\n",
    "    print(f'Using ``--strand`` overwrite option for all the samples {strand[0]}') \n",
    "    strand = [strand] * len(step_strand_detected)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alert-render",
   "metadata": {
    "kernel": "SoS",
    "toc-hr-collapsed": true
   },
   "source": [
    "## Step 4: Mark duplicates reads & QC through `Picard`\n",
    "\n",
    "This step is the first QC step after `STAR` alignment. This step will performed QC to collect multipe metrics regarding the RNASeq using Picard. Then it will also generate a new `.bam` file with duplication marked with the hexadecimal value of `0x0400`, which corresponds to a decimal value of 1024\n",
    " \n",
    "### Step Inputs:\n",
    "\n",
    "* `STAR_bam`: path to the output in Step 2.\n",
    "* `reference_fasta`:  The fasta reference file used to generate star index, it is critical that the this fasta is *exactly* the same as those that generate the STAR Index, else this error will occurs: https://github.com/cumc/xqtl-pipeline/issues/357\n",
    "\n",
    "* `RefFlat file` \n",
    "\n",
    "This file is needed for picard CollectRnaSeqMetrics module, which in turn\n",
    "> produces metrics describing the distribution of the bases within the transcripts. It calculates the total numbers and the fractions of nucleotides within specific genomic regions including untranslated regions (UTRs), introns, intergenic sequences (between discrete genes), and peptide-coding sequences (exons). This tool also determines the numbers of bases that pass quality filters that are specific to Illumina data (PF_BASES).\n",
    "\n",
    "The refFlat file can be generated by the reference_data module, RefFlat_generation step.\n",
    "\n",
    "### Step Outputs:\n",
    "\n",
    "* A collection of metrics file for each of the samples\n",
    "* A new `.bam` file with duplication marked with the hexadecimal value of `0x0400`, which corresponds to a decimal value of 1024\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "antique-rally",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[picard_qc, STAR_align_2]\n",
    "# Reference gene model\n",
    "parameter: gtf = path\n",
    "depends: sos_variable('strand')\n",
    "# Path to flat reference file, for computing QC metric\n",
    "parameter: ref_flat = path\n",
    "# The fasta reference file used to generate star index\n",
    "parameter: reference_fasta = path\n",
    "# For the patterned flowcell models (HiSeq X), change to 2500\n",
    "parameter: optical_distance = 100\n",
    "parameter: varVCFfile = path(\".\")\n",
    "picard_strand_dict = {\"rf\":\"SECOND_READ_TRANSCRIPTION_STRAND\",\"fr\": \"FIRST_READ_TRANSCRIPTION_STRAND\",\"unstranded\":\"NONE\" }\n",
    "input: output_from(\"STAR_align_1\"),group_by = 2, group_with = {\"sample_id\",\"strand\"}\n",
    "output: picard_metrics = f'{_input[0]:nnnn}.alignment_summary_metrics',\n",
    "        picard_rna_metrics = f'{_input[0]:nnnn}.rna_metrics',\n",
    "        md_bam = f'{_input[0]:n}.md.bam',\n",
    "        md_metrics = f'{_input[0]:n}.md.metrics'\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime, mem = mem, cores = numThreads\n",
    "bash: container=container, expand= \"${ }\", stderr = f'{_output[0]:n}.stderr', stdout = f'{_output[0]:n}.stdout', entrypoint=entrypoint\n",
    "        set -e\n",
    "        touch ${_output[0]:n}.CollectMultipleMetrics_start.timestamp\n",
    "        picard -Xmx${java_mem} CollectMultipleMetrics \\\n",
    "            -REFERENCE_SEQUENCE ${reference_fasta} \\\n",
    "            -PROGRAM CollectAlignmentSummaryMetrics \\\n",
    "            -PROGRAM CollectInsertSizeMetrics \\\n",
    "            -PROGRAM QualityScoreDistribution \\\n",
    "            -PROGRAM MeanQualityByCycle \\\n",
    "            -PROGRAM CollectBaseDistributionByCycle \\\n",
    "            -PROGRAM CollectGcBiasMetrics \\\n",
    "            -VALIDATION_STRINGENCY STRICT \\\n",
    "            -INPUT  ${_input[\"cord_bam\"]} \\\n",
    "            -OUTPUT  ${_output[0]:n}\n",
    "\n",
    "bash: container=container, expand= \"${ }\", stderr = f'{_output[-1]:n}.stderr', stdout = f'{_output[-1]:n}.stdout', entrypoint=entrypoint\n",
    "        set -e\n",
    "        touch ${_output[2]:n}.MarkDuplicates_start.timestamp\n",
    "        picard -Xmx${java_mem}  MarkDuplicates \\\n",
    "            -I ${_input[\"cord_bam\"]}  \\\n",
    "            -O ${_output[2]} \\\n",
    "            -PROGRAM_RECORD_ID null \\\n",
    "            -M ${_output[3]} \\\n",
    "            -TMP_DIR ${cwd}\\\n",
    "            -MAX_RECORDS_IN_RAM 500000 -SORTING_COLLECTION_SIZE_RATIO 0.25 \\\n",
    "            -ASSUME_SORT_ORDER coordinate \\\n",
    "            -TAGGING_POLICY DontTag \\\n",
    "            -OPTICAL_DUPLICATE_PIXEL_DISTANCE ${optical_distance} \\\n",
    "            -CREATE_INDEX true \\\n",
    "            -CREATE_MD5_FILE true \\\n",
    "            -VALIDATION_STRINGENCY STRICT \\\n",
    "            -REMOVE_SEQUENCING_DUPLICATES false \\\n",
    "            -REMOVE_DUPLICATES false \n",
    "\n",
    "bash: container=container, expand= \"${ }\", stderr = f'{_output[0]:n}.rna_metrics.stderr', stdout = f'{_output[0]:n}.rna_metrics.stderr', entrypoint=entrypoint\n",
    "        set -e\n",
    "        # Get only line with rRNA and transcript_id\n",
    "        cat ${gtf}| grep rRNA | grep transcript_id > ${gtf}.tmp.${_input[\"cord_bam\"]:bnnnn}  # To Avoid data racing problem\n",
    "        samtools view -H ${_input[\"cord_bam\"]}  > ${_input[\"cord_bam\"]}.RI  \n",
    "\n",
    "python: container=container, expand= \"${ }\", stderr = f'{_output[0]:n}.rna_metrics.stderr', stdout = f'{_output[0]:n}.rna_metrics.stderr', entrypoint=entrypoint\n",
    "        import pandas as pd\n",
    "        from collections import defaultdict\n",
    "        chrom = []\n",
    "        start = []\n",
    "        end = []\n",
    "        strand = []\n",
    "        tag = []\n",
    "        annotation_gtf = \"${gtf}.tmp.${_input[\"cord_bam\"]:bnnnn}\"\n",
    "        with open(annotation_gtf, 'r') as gtf:\n",
    "            for row in gtf:\n",
    "                row = row.strip().split('\\t')\n",
    "                if row[0][0]=='#' or row[2]!=\"transcript\": continue # skip header\n",
    "                chrom.append(row[0])\n",
    "                start.append(row[3])\n",
    "                end.append(row[4])\n",
    "                strand.append(row[6])\n",
    "                attributes = defaultdict()\n",
    "                for a in row[8].replace('\"', '').split(';')[:-1]:\n",
    "                    kv = a.strip().split(' ')\n",
    "                    if kv[0]!='tag':\n",
    "                        attributes[kv[0]] = kv[1]\n",
    "                    else:\n",
    "                        attributes.setdefault('tags', []).append(kv[1])\n",
    "                tag.append(attributes)\n",
    "        transcript_id = [x[\"transcript_id\"] for x in tag]\n",
    "        RI = pd.DataFrame(data={'chr':chrom, 'start':start, 'end':end, 'strand':strand,'transcript_id' : transcript_id })\n",
    "        RI.to_csv(\"${_input[\"cord_bam\"]}.RI\", index = 0, header = 0, mode = \"a\",sep = \"\\t\" )\n",
    "\n",
    "bash: container=container, expand= \"${ }\", stderr = f'{_output[0]:n}.rna_metrics.stderr', stdout = f'{_output[0]:n}.rna_metrics.stdout', entrypoint=entrypoint\n",
    "        set -e\n",
    "        touch ${_output[0]:n}.CollectRnaSeqMetrics_start.timestamp\n",
    "        picard -Xmx${java_mem} CollectRnaSeqMetrics \\\n",
    "            -REF_FLAT ${ref_flat} \\\n",
    "            -RIBOSOMAL_INTERVALS ${_input[\"cord_bam\"]}.RI \\\n",
    "            -STRAND_SPECIFICITY ${picard_strand_dict[_strand]} \\\n",
    "            -CHART_OUTPUT ${_output[0]:n}.rna_metrics.pdf \\\n",
    "            -VALIDATION_STRINGENCY STRICT \\\n",
    "            -INPUT ${_input[\"cord_bam\"]}  \\\n",
    "            -OUTPUT  ${_output[0]:n}.rna_metrics\n",
    "        rm ${gtf}.tmp.${_input[\"cord_bam\"]:bnnnn}\n",
    "\n",
    "_input[\"cord_bam\"].zap()\n",
    "\n",
    "bash: container=container, expand= \"$[ ]\", stderr = f'{_output[0]:n}.bw.stderr', stdout = f'{_output[0]:n}.bw.stdout', entrypoint=entrypoint\n",
    "    generate_bigwig() {\n",
    "        # Check if enough arguments are provided\n",
    "        if [[ $# -ne 1 ]]; then\n",
    "            echo \"Usage: generate_bigwig <file.bam>\"\n",
    "            return 1\n",
    "        fi\n",
    "\n",
    "        local input_file=\"$1\"\n",
    "        \n",
    "        # Derive file.sorted.bw from file.bam\n",
    "        local bigwig_file=\"${input_file%.*}.sorted.bw\"\n",
    "\n",
    "        # Execute the commands\n",
    "        samtools index \"$input_file\"\n",
    "        bamCoverage -b \"$input_file\" -o \"$bigwig_file\"\n",
    "    }\n",
    "\n",
    "    export -f generate_bigwig  # To make it available in subshells, if needed.\n",
    "    $[ f'generate_bigwig {_output[2]}' if  varVCFfile.is_file() else \"\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ruled-dakota",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[STAR_align_3]\n",
    "input: group_by = \"all\"\n",
    "depends: sos_variable(\"strand\")\n",
    "output: f'{cwd}/{samples:bn}_bam_list'\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime, mem = mem, cores = numThreads\n",
    "python: container=container, expand= \"${ }\", entrypoint=entrypoint\n",
    "    import pandas as pd\n",
    "    coord_bam_list = [${_input:br,}][2::4]\n",
    "    SJ_list = [f'{x}.SJ.out.tab' for x in [${_input:bnnnnnr,}][2::4]] \n",
    "    Trans_bam_list = [f'{x}.toTranscriptome.out.bam' for x in [${_input:bnnnnr,}][2::4]]\n",
    "    out = pd.DataFrame({\"sample_id\" : ${sample_id},\"strand\" : ${strand} , \"coord_bam_list\" : coord_bam_list, \"SJ_list\" :SJ_list,\"trans_bam_list\" : Trans_bam_list  })\n",
    "    out.to_csv(\"${_output}\", sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stopped-retreat",
   "metadata": {
    "kernel": "SoS",
    "toc-hr-collapsed": true
   },
   "source": [
    "## Step 5: Post aligment QC through `RNA-SeQC`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assured-spectacular",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "Documentation : [RNA-SeQC](https://github.com/getzlab/rnaseqc) and [Script in docker](https://github.com/broadinstitute/gtex-pipeline/blob/master/rnaseq/src/run_rnaseqc.py)\n",
    "\n",
    "This step is second QC step after `STAR` alignment. It will perform RNA-seq quantification as well. \n",
    "\n",
    "### Step Inputs\n",
    "\n",
    "* `bam_list`: path to the output of STAR_output, which outlined the strands and bam file used for each samples.\n",
    "* `gtf`: reference genome `.gtf` file, this gtf file need to have the same chr name format as the index used to generate the bam file and must be on collapsed gene gtf \n",
    "\n",
    "### Step Outputs\n",
    "\n",
    "\n",
    "The following output files are generated in the output directory you provide:\n",
    "\n",
    "* {sample}.metrics.tsv : A tab-delimited list of (Statistic, Value) pairs of all statistics and metrics recorded.\n",
    "\n",
    "* {sample}.exon_reads.gct : A tab-delimited GCT file with (Exon ID, Gene Name, coverage) tuples for all exons which had at least part of one read mapped.\n",
    "\n",
    "* {sample}.gene_reads.gct : A tab-delimited GCT file with (Gene ID, Gene Name, coverage) tuples for all genes which had at least one read map to at least one of its exons\n",
    "\n",
    "* {sample}.gene_tpm.gct : A tab-delimited GCT file with (Gene ID, Gene Name, TPM) tuples for all genes reported in the gene_reads.gct file. Note: this file is renamed to .gene_rpkm.gct if the --rpkm flag is present.\n",
    "\n",
    "* {sample}.fragmentSizes.txt : A list of fragment sizes recorded, if a BED file was provided\n",
    "\n",
    "* {sample}.coverage.tsv : A tab-delimited list of (Gene ID, Transcript ID, Mean Coverage, Coverage Std, Coverage CV) tuples for all transcripts encountered in the GTF.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "liked-legend",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[rnaseqc_call_1]\n",
    "import os\n",
    "import pandas as pd\n",
    "parameter: bam_list = path\n",
    "# Reference gene model\n",
    "parameter: gtf = path\n",
    "sample_inv = pd.read_csv(bam_list,sep = \"\\t\")\n",
    "parameter: detection_threshold = 5\n",
    "parameter: mapping_quality = 255\n",
    "## Extract strand information if user have specified the strand\n",
    "strand_list = sample_inv.strand.values.tolist()\n",
    "stop_if(not all([x in [\"fr\", \"rf\", \"unstranded\"] for x in strand_list ]), msg = \"strand columns should only include ``fr``, ``rf`` or ``unstranded``, please check the bam_list\")     \n",
    "## Extract sample_id\n",
    "sample_id = sample_inv.sample_id.values.tolist()\n",
    "    \n",
    "## Get the file name for cood_bam data\n",
    "coord_bam_list_inv = sample_inv.coord_bam_list.values.tolist()\n",
    "coord_bam_list = [f'{cwd}/{x}' for x in coord_bam_list_inv ]\n",
    "input:  coord_bam_list, group_by = 1, group_with = {\"sample_id\",\"strand_list\"}\n",
    "output: f'{cwd}/{_sample_id}.rnaseqc.gene_tpm.gct.gz',\n",
    "        f'{cwd}/{_sample_id}.rnaseqc.gene_reads.gct.gz',\n",
    "        f'{cwd}/{_sample_id}.rnaseqc.exon_reads.gct.gz',\n",
    "        f'{cwd}/{_sample_id}.rnaseqc.metrics.tsv'\n",
    "\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime, mem = mem, cores = numThreads\n",
    "bash: container=container, expand= \"${ }\", stderr = f'{_output[0]:n}.stderr', stdout = f'{_output[0]:n}.stdout', entrypoint=entrypoint\n",
    "    cd ${cwd} && \\\n",
    "    rnaseqc \\\n",
    "        ${gtf:a} \\\n",
    "        ${_input:a} \\\n",
    "        ${_output[0]:d}  \\\n",
    "        ${(\"--stranded \" + _strand_list) if _strand_list != \"unstranded\" else \"\"} \\\n",
    "        ${f'--detection-threshold {detection_threshold}'} ${f'--mapping-quality {mapping_quality}'} ${ '' if is_paired_end else '-u'}\n",
    "\n",
    "bash: container=container, expand= \"${ }\", stderr = f'{_output[0]:n}.stderr', stdout = f'{_output[0]:n}.stdout', entrypoint=entrypoint\n",
    "    mv ${_output[0]:d}/${_input:b}.gene_tpm.gct ${_output[0]:n}\n",
    "    mv ${_output[0]:d}/${_input:b}.gene_reads.gct ${_output[1]:n}\n",
    "    mv ${_output[0]:d}/${_input:b}.exon_reads.gct ${_output[2]:n}\n",
    "    mv ${_output[0]:d}/${_input:b}.metrics.tsv ${_output[3]}\n",
    "    gzip ${_output[0]:n} ${_output[1]:n} ${_output[2]:n}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "packed-cleveland",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "The RNASEQC results were merged in the following step,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "former-chart",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[rnaseqc_call_2]\n",
    "input: group_by = \"all\"\n",
    "output: f'{cwd}/{samples:bn}.rnaseqc.gene_tpm.gct.gz',\n",
    "        f'{cwd}/{samples:bn}.rnaseqc.gene_readsCount.gct.gz',\n",
    "        f'{cwd}/{samples:bn}.rnaseqc.exon_readsCount.gct.gz',\n",
    "        f'{cwd}/{samples:bn}.rnaseqc.metrics.tsv'\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime, mem = mem, cores = numThreads\n",
    "python: container=container, expand= \"${ }\", stderr = f'{_output[0]:n}.stderr', stdout = f'{_output[0]:n}.stdout', entrypoint=entrypoint\n",
    "    import pandas as pd\n",
    "    import os\n",
    "    def make_gct(gct_path):\n",
    "        # sample_name\n",
    "        sample_name = \".\".join(os.path.basename(gct_path).split(\".\")[:-4])\n",
    "        # read_input\n",
    "        pre_gct = pd.read_csv(gct_path,sep = \"\\t\",\n",
    "                              skiprows= 2,index_col=\"Name\").drop(\"Description\",axis = 1)\n",
    "        pre_gct.index.name = \"gene_ID\"\n",
    "        pre_gct.columns = [sample_name]\n",
    "        return(pre_gct)\n",
    "\n",
    "    def merge_gct(gct_path_list):\n",
    "        gct = pd.DataFrame()\n",
    "        for gct_path in gct_path_list:\n",
    "            #check duplicated indels and remove them.\n",
    "            gct_col = make_gct(gct_path)\n",
    "            gct = gct.merge(gct_col,right_index=True,left_index = True,how = \"outer\")\n",
    "        return gct\n",
    "\n",
    "    input_list = [${_input:r,}]\n",
    "    tpm_list = input_list[0::4]\n",
    "    gc_list = input_list[1::4]\n",
    "    ec_list = input_list[2::4]\n",
    "    gct_path_list_list = [tpm_list,gc_list,ec_list]\n",
    "    output_path = [${_output:r,}][0:3]\n",
    "    for i in range(0,len(output_path)):\n",
    "        output = merge_gct(gct_path_list_list[i])\n",
    "        output.to_csv(output_path[i], sep = \"\\t\")\n",
    "    metrics_list = input_list[3::4]\n",
    "    with open(\"${cwd}/${samples:bn}.rnaseqc.metrics_output_list\", \"w\") as f:\n",
    "        f.write('\\n'.join(metrics_list))\n",
    "\n",
    "bash: container=container, expand= \"${ }\", stderr = f'{_output[0]:n}.stderr', stdout = f'{_output[0]:n}.stdout', entrypoint=entrypoint\n",
    "    aggregate_rnaseqc_metrics.py  ${_output[3]:n}_output_list ${_output[3]:nn}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "humanitarian-planning",
   "metadata": {
    "kernel": "SoS",
    "toc-hr-collapsed": true
   },
   "source": [
    "## Step 6: Quantify expression through `RSEM`\n",
    "\n",
    "Documentation : [RSEM](https://deweylab.github.io/RSEM/rsem-calculate-expression.html) and [Script in docker](https://github.com/broadinstitute/gtex-pipeline/blob/master/rnaseq/src/run_RSEM.py)\n",
    "\n",
    "This step generate the expression matrix from STAR output. Estimate gene and isoform expression from RNA-Seq data are generated.\n",
    "\n",
    "### Step Input\n",
    "\n",
    "* `bam_list`: path to the output of STAR_output, which outlined the strands and bam file used for each samples.\n",
    "* transcript-level BAM file: path to the output of Step 3.\n",
    "* `RSEM_index`: path to RSEM index\n",
    "\n",
    "### Step Outputs\n",
    "Please see the output section of https://deweylab.github.io/RSEM/rsem-calculate-expression.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "appropriate-special",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[rsem_call_1]\n",
    "parameter: RSEM_index = path\n",
    "parameter: max_frag_len = 1000\n",
    "estimate_rspd = True\n",
    "\n",
    "parameter: bam_list = path\n",
    "\n",
    "sample_inv = pd.read_csv(bam_list,sep = \"\\t\")\n",
    "\n",
    "## Extract strand information if user have specified the strand\n",
    "strand_list = sample_inv.strand.values.tolist()\n",
    "stop_if(not all([x in [\"fr\", \"rf\", \"unstranded\"] for x in strand_list ]), msg = \"strand columns should only include ``fr``, ``rf`` or ``unstranded``, please check the bam_list\")     \n",
    "## Extract sample_id\n",
    "sample_id = sample_inv.sample_id.values.tolist()\n",
    "    \n",
    "## Get the file name for trans_bam_list data\n",
    "trans_bam_list_inv = sample_inv.trans_bam_list.values.tolist()\n",
    "trans_bam_list = [f'{cwd}/{x}' for x in trans_bam_list_inv ]\n",
    "input: trans_bam_list ,  group_by = 1, group_with = {\"sample_id\",\"strand_list\"} \n",
    "output: f'{cwd}/{_sample_id}.rsem.isoforms.results', f'{cwd}/{_sample_id}.rsem.genes.results',f'{cwd}/{_sample_id}.rsem.stat/{_sample_id}.rsem.cnt'\n",
    "task: trunk_workers = 1, walltime = walltime, mem = mem, cores = numThreads, trunk_size = job_size\n",
    "bash: container=container, expand= \"${ }\", stderr = f'{_output[0]:n}.stderr', stdout = f'{_output[0]:n}.stdout', entrypoint=entrypoint\n",
    "    run_RSEM.py ${RSEM_index:a} ${_input:a} ${_sample_id} \\\n",
    "        -o ${_output[0]:d} \\\n",
    "        --max_frag_len ${max_frag_len} \\\n",
    "        --estimate_rspd ${'true' if estimate_rspd else 'false'} \\\n",
    "        --paired_end ${\"true\" if is_paired_end else \"false\"} \\\n",
    "        --is_stranded ${\"true\" if _strand_list != \"unstranded\" else \"false\"} \\\n",
    "        --threads ${numThreads}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "entitled-swimming",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "The RSEM results were merged in the following steps, seven files (four for each columns in the isoform output and 3 for each of the genes output) will be generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "marked-buffalo",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[rsem_call_2]\n",
    "input: group_by = \"all\"\n",
    "output: f'{cwd}/{samples:bn}.rsem_transcripts_expected_count.txt.gz',\n",
    "        f'{cwd}/{samples:bn}.rsem_transcripts_tpm.txt.gz',\n",
    "        f'{cwd}/{samples:bn}.rsem_transcripts_fpkm.txt.gz',\n",
    "        f'{cwd}/{samples:bn}.rsem_transcripts_isopct.txt.gz',\n",
    "        f'{cwd}/{samples:bn}.rsem_genes_expected_count.txt.gz',\n",
    "        f'{cwd}/{samples:bn}.rsem_genes_tpm.txt.gz',\n",
    "        f'{cwd}/{samples:bn}.rsem_genes_fpkm.txt.gz',\n",
    "        f'{cwd}/{samples:bn}.rsem.aggregated_quality.metrics.tsv'\n",
    "\n",
    "task: trunk_workers = 1, walltime = walltime, mem = mem, cores = numThreads, trunk_size = job_size\n",
    "python: container=container, expand= \"${ }\", stderr = f'{_output[0]:n}.stderr', stdout = f'{_output[0]:n}.stdout', entrypoint=entrypoint\n",
    "    input_list = [${_input:r,}]\n",
    "    with open('${cwd}/${samples:bn}.rsem.isoforms_output_list', \"w\") as f:\n",
    "        f.write('\\n'.join(input_list[0::3]))\n",
    "    with open('${cwd}/${samples:bn}.rsem.genes_output_list', \"w\") as f:\n",
    "        f.write('\\n'.join(input_list[1::3]))\n",
    "\n",
    "bash: container=container, expand= \"${ }\", stderr = f'{_output[0]:n}.stderr', stdout = f'{_output[0]:n}.stdout', entrypoint=entrypoint\n",
    "         aggregate_rsem_results.py ${cwd}/${samples:bn}.rsem.isoforms_output_list {expected_count,TPM,FPKM,IsoPct} ${_output[0]:nnn}\n",
    "         aggregate_rsem_results.py ${cwd}/${samples:bn}.rsem.genes_output_list {expected_count,TPM,FPKM} ${_output[1]:nnn} \n",
    "\n",
    "R:  container=container, expand= \"${ }\", stderr = f'{_output[-1]:n}.stderr', stdout = f'{_output[-1]:n}.stdout', entrypoint=entrypoint\n",
    "     readRSEM.cnt <- function (source) {\n",
    "            # RSEM .cnt files gives statistics about the (transcriptome) alignment passed to RSEM:\n",
    "            # Row 1: N0 (# unalignable reads);\n",
    "            #        N1 (# alignable reads);\n",
    "            #        N2 (# filtered reads due to too many alignments);\n",
    "            #        N_tot (N0+N1+N2)\n",
    "            # Row 2: nUnique (# reads aligned uniquely to a gene);\n",
    "            #        nMulti (# reads aligned to multiple genes);\n",
    "            #        nUncertain (# reads aligned to multiple locations in the given reference sequences, which include isoform-level multi-mapping reads)\n",
    "            # Row 3: nHits (# total alignments);\n",
    "            #        read_type (0: single-end read, no quality; 1: single-end read, with quality score; 2: paired-end read, no quality score; 3: paired-end read, with quality score)\n",
    "            # Source: https://groups.google.com/forum/#!topic/rsem-users/usmPKgsC5LU\n",
    "            # Note: N1 = nUnique + nMulti\n",
    "\n",
    "            stopifnot(file.exists(source[1]))\n",
    "            isDir <- file.info(source)$isdir\n",
    "            if (isDir) {\n",
    "                files <- system(paste(\"find\", source, \"-name \\\"*.rsem.cnt\\\"\"), intern=TRUE)\n",
    "                stopifnot(length(files) > 0)\n",
    "                samples <- gsub(\"_rsem.cnt\", \"\", basename(files), fixed=TRUE)\n",
    "            } else {\n",
    "                files <- source\n",
    "                samples <- gsub(\".rsem.cnt\", \"\", basename(files), fixed=TRUE)\n",
    "            }\n",
    "            metrics <- list()\n",
    "            for (i in 1:length(files)) {\n",
    "                m <- read.table(files[i], header=FALSE, sep=\" \", comment.char=\"#\", stringsAsFactors=FALSE, nrows=3, fill=TRUE)\n",
    "                metrics[[i]] <- data.frame(Sample=samples[i],\n",
    "                File=files[i],\n",
    "                TotalReads=m[1, 4],\n",
    "                AlignedReads=m[1, 2],\n",
    "                UniquelyAlignedReads=m[2, 1],\n",
    "                stringsAsFactors=FALSE)\n",
    "            }\n",
    "            metrics <- do.call(rbind, metrics)\n",
    "            row.names(metrics) <- metrics$Sample\n",
    "\n",
    "            return(metrics)\n",
    "        }\n",
    "        sourceRSEM = c(${_input:r,})\n",
    "        sourceRSEM = sourceRSEM[seq(3,length(sourceRSEM),3)]\n",
    "        metrics.RSEM = readRSEM.cnt(sourceRSEM)\n",
    "        write.table(metrics.RSEM, file=\"${_output[-1]}\",col.names=TRUE, row.names=FALSE, quote=FALSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acting-cholesterol",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Step 7: summarize with MultiQC\n",
    "\n",
    "[MultiQC](https://multiqc.info/docs/#using-multiqc) \n",
    "\n",
    "\n",
    ">MultiQC is a reporting tool that parses summary statistics from results and log files generated by other bioinformatics tools. MultiQC doesn't run other tools for you - it's designed to be placed at the end of analysis pipelines or to be run manually when you've finished running your tools. When you launch MultiQC, it recursively searches through any provided file paths and finds files that it recognises. It parses relevant information from these and generates a single stand-alone HTML report file. It also saves a directory of data files with all parsed data for further downstream use.\n",
    "\n",
    "MultiQC will automatically generate QC report for anything embedded within the given directory. Therefore providing the directory containing all the output will surfice.\n",
    "\n",
    "The output of MultiQC is a multi-module report each corresponding to the quality report of each step of analysis previously performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "crude-nursery",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[rsem_call_3,rnaseqc_call_3]\n",
    "output: f'{cwd}/{samples:bn}.multiqc_report.html'\n",
    "task: trunk_workers = 1, walltime = walltime, mem = mem, cores = numThreads, trunk_size = job_size\n",
    "report: output = f\"{_output:n}.multiqc_config.yml\"\n",
    "  extra_fn_clean_exts:\n",
    "      - '_rsem'\n",
    "  fn_ignore_dirs:\n",
    "      - '*_STARpass1'\n",
    "bash:  container=container,expand= \"${ }\", stderr = f'{_output:n}.stderr', stdout = f'{_output:n}.stdout', entrypoint=entrypoint\n",
    "    multiqc ${_input:d} -v -n ${_output:b} -o ${_output:d} -c ${_output:n}.multiqc_config.yml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "female-darwin",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[rsem_call_4, rnaseqc_call_4]\n",
    "# Path to flat reference file, for computing QC metric\n",
    "output: f'{cwd}/{samples:b}.picard.aggregated_quality.metrics.tsv'\n",
    "task: trunk_workers = 1, walltime = walltime, mem = mem, cores = numThreads, trunk_size = job_size\n",
    "R: container=container, expand= \"${ }\", stderr = f'{_output:n}.stderr', stdout = f'{_output:n}.stdout', entrypoint=entrypoint\n",
    "    ## Define Function\n",
    "    readPicard.alignment_summary_metrics <- function (source) {\n",
    "      stopifnot(length(source) == 1)\n",
    "      stopifnot(file.exists(source))\n",
    "      isDir <- file.info(source)$isdir\n",
    "      if (isDir) {\n",
    "        files <- system(paste(\"find -L\", source, \"-name \\\"*.alignment_summary_metrics\\\"\"), intern=TRUE)\n",
    "        stopifnot(length(files) > 0)\n",
    "        samples <- gsub(\".alignment_summary_metrics\", \"\", basename(files), fixed=TRUE)\n",
    "      } else {\n",
    "        files <- source\n",
    "        samples <- gsub(\".alignment_summary_metrics\", \"\", basename(files), fixed=TRUE)\n",
    "      }\n",
    "    \n",
    "      metrics <- list()\n",
    "      for (i in 1:length(files)) {\n",
    "        m <- read.table(files[i], header=TRUE, sep=\"\\t\", comment.char=\"#\", stringsAsFactors=FALSE, nrows=${is_paired_end}+1)\n",
    "        metrics[[i]] <- data.frame(Sample=samples[i],\n",
    "                                   File=files[i],\n",
    "                                   PF_READS=sum(m$PF_READS[1:2]),\n",
    "                                   PF_READS_ALIGNED=sum(m$PF_READS_ALIGNED[1:2]),\n",
    "                                   PCT_PF_READS_ALIGNED=sum(m$PF_READS_ALIGNED[1:2])/sum(m$PF_READS[1:2]),\n",
    "                                   stringsAsFactors=FALSE)\n",
    "      }\n",
    "      metrics <- do.call(rbind, metrics)\n",
    "      row.names(metrics) <- metrics$Sample\n",
    "    \n",
    "      return(metrics)\n",
    "    }\n",
    "\n",
    "    readPicard.rna_metrics <- function(source) {\n",
    "\n",
    "      stopifnot(length(source) == 1)\n",
    "      stopifnot(file.exists(source))\n",
    "      isDir <- file.info(source)$isdir\n",
    "      if (isDir) {\n",
    "        files <- system(paste(\"find -L\", source, \"-name \\\"*.rna_metrics\\\"\"), intern=TRUE)\n",
    "        stopifnot(length(files) > 0)\n",
    "        samples <- gsub(\".rna_metrics\", \"\", basename(files), fixed=TRUE)\n",
    "      } else {\n",
    "        files <- source\n",
    "        samples <- gsub(\".rna_metrics\", \"\", basename(files), fixed=TRUE)\n",
    "      }\n",
    "\n",
    "      metrics <- list()\n",
    "      for (i in 1:length(files)) {\n",
    "        m <- read.table(files[i], header=TRUE, sep=\"\\t\", comment.char=\"#\", stringsAsFactors=FALSE, nrows=1)\n",
    "        metrics[[i]] <- data.frame(Sample=samples[i],\n",
    "                                   File=files[i],\n",
    "                                   PCT_RIBOSOMAL_BASES=m$PCT_RIBOSOMAL_BASES,\n",
    "                                   PCT_CODING_BASES=m$PCT_CODING_BASES,\n",
    "                                   PCT_UTR_BASES=m$PCT_UTR_BASES,\n",
    "                                   PCT_INTRONIC_BASES=m$PCT_INTRONIC_BASES,\n",
    "                                   PCT_INTERGENIC_BASES=m$PCT_INTERGENIC_BASES,\n",
    "                                   PCT_MRNA_BASES=m$PCT_MRNA_BASES,\n",
    "                                   PCT_USABLE_BASES=m$PCT_USABLE_BASES,\n",
    "                                   MEDIAN_CV_COVERAGE=m$MEDIAN_CV_COVERAGE,\n",
    "                                   MEDIAN_5PRIME_BIAS=m$MEDIAN_5PRIME_BIAS,\n",
    "                                   MEDIAN_3PRIME_BIAS=m$MEDIAN_3PRIME_BIAS,\n",
    "                                   MEDIAN_5PRIME_TO_3PRIME_BIAS=m$MEDIAN_5PRIME_TO_3PRIME_BIAS,\n",
    "                                   stringsAsFactors=FALSE)\n",
    "      }\n",
    "      metrics <- do.call(rbind, metrics)\n",
    "      row.names(metrics) <- metrics$Sample\n",
    "    \n",
    "      return(metrics)\n",
    "    }\n",
    "\n",
    "\n",
    "    readPicard.duplicate_metrics <- function(source) {\n",
    "\n",
    "      stopifnot(length(source) == 1)\n",
    "      stopifnot(file.exists(source))\n",
    "      isDir <- file.info(source)$isdir\n",
    "      if (isDir) {\n",
    "        files <- system(paste(\"find -L\", source, \"-name \\\"*.Aligned.sortedByCoord.md.metrics\\\"\"), intern=TRUE)\n",
    "        stopifnot(length(files) > 0)\n",
    "        samples <- gsub(\".Aligned.sortedByCoord.md.metrics\", \"\", basename(files), fixed=TRUE)\n",
    "      } else {\n",
    "        files <- source\n",
    "        samples <- gsub(\".Aligned.sortedByCoord.md.metrics\", \"\", basename(files), fixed=TRUE)\n",
    "      }\n",
    "\n",
    "      metrics <- list()\n",
    "      for (i in 1:length(files)) {\n",
    "        m <- read.table(files[i], header=TRUE, sep=\"\\t\", comment.char=\"#\", stringsAsFactors=FALSE, nrows=1)\n",
    "        metrics[[i]] <- data.frame(Sample=samples[i],\n",
    "                                   File=files[i],\n",
    "                                   PERCENT_DUPLICATION=m$PERCENT_DUPLICATION,\n",
    "                                   ESTIMATED_LIBRARY_SIZE=m$ESTIMATED_LIBRARY_SIZE,\n",
    "                                   stringsAsFactors=FALSE)\n",
    "      }\n",
    "      metrics <- do.call(rbind, metrics)\n",
    "      row.names(metrics) <- metrics$Sample\n",
    "    \n",
    "      return(metrics)\n",
    "    }\n",
    "\n",
    "    readPicard.wgs_metrics <- function (source) {\n",
    "\n",
    "      stopifnot(length(source) == 1)\n",
    "      stopifnot(file.exists(source))\n",
    "      isDir <- file.info(source)$isdir\n",
    "      if (isDir) {\n",
    "        files <- system(paste(\"find -L\", source, \"-name \\\"*.wgs_metrics\\\"\"), intern=TRUE)\n",
    "        stopifnot(length(files) > 0)\n",
    "        samples <- gsub(\".wgs_metrics\", \"\", basename(files), fixed=TRUE)\n",
    "      } else {\n",
    "        files <- source\n",
    "        samples <- gsub(\".wgs_metrics\", \"\", basename(files), fixed=TRUE)\n",
    "      }\n",
    "\n",
    "      metrics <- list()\n",
    "      for (i in 1:length(files)) {\n",
    "        m <- read.table(files[i], header=TRUE, sep=\"\\t\", comment.char=\"#\", stringsAsFactors=FALSE, nrows=1)\n",
    "        metrics[[i]] <- data.frame(Sample=samples[i],\n",
    "                                   File=files[i],\n",
    "                                   MEDIAN_COVERAGE=m$MEDIAN_COVERAGE,\n",
    "                                   MAD_COVERAGE=m$MAD_COVERAGE,\n",
    "                                   PCT_EXC_DUPE=m$PCT_EXC_DUPE,\n",
    "                                   PCT_EXC_TOTAL=m$PCT_EXC_TOTAL,\n",
    "                                   stringsAsFactors=FALSE)\n",
    "      }\n",
    "      metrics <- do.call(rbind, metrics)\n",
    "      row.names(metrics) <- metrics$Sample\n",
    "\n",
    "      return(metrics)\n",
    "    }\n",
    "\n",
    "\n",
    "    readPicard.insert_size_metrics <- function (source) {\n",
    "\n",
    "      stopifnot(length(source) == 1)\n",
    "      stopifnot(file.exists(source))\n",
    "      isDir <- file.info(source)$isdir\n",
    "      if (isDir) {\n",
    "        files <- system(paste(\"find -L\", source, \"-name \\\"*.insert_size_metrics\\\"\"), intern=TRUE)\n",
    "        stopifnot(length(files) > 0)\n",
    "        samples <- gsub(\".insert_size_metrics\", \"\", basename(files), fixed=TRUE)\n",
    "      } else {\n",
    "        files <- source\n",
    "        samples <- gsub(\".insert_size_metrics\", \"\", basename(files), fixed=TRUE)\n",
    "      }\n",
    "\n",
    "      metrics <- list()\n",
    "      for (i in 1:length(files)) {\n",
    "        m <- read.table(files[i], header=TRUE, sep=\"\\t\", comment.char=\"#\", stringsAsFactors=FALSE, nrows=1)\n",
    "        metrics[[i]] <- data.frame(Sample=samples[i],\n",
    "                                   File=files[i],\n",
    "                                   MEDIAN_INSERT_SIZE=m$MEDIAN_INSERT_SIZE,\n",
    "                                   MODE_INSERT_SIZE=m$MODE_INSERT_SIZE,\n",
    "                                   MEDIAN_ABSOLUTE_DEVIATION=m$MEDIAN_ABSOLUTE_DEVIATION,\n",
    "                                   stringsAsFactors=FALSE)\n",
    "      }\n",
    "      metrics <- do.call(rbind, metrics)\n",
    "      row.names(metrics) <- metrics$Sample\n",
    "    \n",
    "      return(metrics)\n",
    "    }\n",
    "\n",
    "    readPicard.gc_bias.summary_metrics <- function (source) {\n",
    "\n",
    "      stopifnot(length(source) == 1)\n",
    "      stopifnot(file.exists(source))\n",
    "      isDir <- file.info(source)$isdir\n",
    "      if (isDir) {\n",
    "        files <- system(paste(\"find -L\", source, \"-name \\\"*.gc_bias.summary_metrics\\\"\"), intern=TRUE)\n",
    "        stopifnot(length(files) > 0)\n",
    "        samples <- gsub(\".gc_bias.summary_metrics\", \"\", basename(files), fixed=TRUE)\n",
    "      } else {\n",
    "        files <- source\n",
    "        samples <- gsub(\".gc_bias.summary_metrics\", \"\", basename(files), fixed=TRUE)\n",
    "      }\n",
    "\n",
    "      metrics <- list()\n",
    "      for (i in 1:length(files)) {\n",
    "        m <- read.table(files[i], header=TRUE, sep=\"\\t\", comment.char=\"#\", stringsAsFactors=FALSE, nrows=1)\n",
    "        metrics[[i]] <- data.frame(Sample=samples[i],\n",
    "                                   File=files[i],\n",
    "                                   AT_DROPOUT=m$AT_DROPOUT,\n",
    "                                   GC_DROPOUT=m$GC_DROPOUT,\n",
    "                                   stringsAsFactors=FALSE)\n",
    "      }\n",
    "      metrics <- do.call(rbind, metrics)\n",
    "      row.names(metrics) <- metrics$Sample\n",
    "\n",
    "      return(metrics)\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    readPicard <- function(source) {\n",
    "      metrics.aln <- readPicard.alignment_summary_metrics(source)\n",
    "      metrics.rna <- readPicard.rna_metrics(source)\n",
    "      metrics.dup <- readPicard.duplicate_metrics(source)\n",
    "\n",
    "      stopifnot(all(row.names(metrics.aln) %in% row.names(metrics.rna)) &\n",
    "                all(row.names(metrics.rna) %in% row.names(metrics.dup)) &\n",
    "                all(row.names(metrics.dup) %in% row.names(metrics.aln)))\n",
    "\n",
    "      metrics.aln$File <- NULL\n",
    "      metrics.rna$File <- NULL\n",
    "      metrics.dup$File <- NULL\n",
    "      metrics.rna$Sample <- NULL\n",
    "      metrics.dup$Sample <- NULL\n",
    "\n",
    "      metrics <- cbind(metrics.aln, metrics.rna[row.names(metrics.aln), ])\n",
    "      metrics <- cbind(metrics, metrics.dup[row.names(metrics.aln), ])\n",
    "\n",
    "      return(metrics)\n",
    "    }\n",
    "\n",
    "    ## Execution  \n",
    "    sourcePicard = ${_input[-1]:dr}\n",
    "\n",
    "    Picard_qualityMetrics <- readPicard(sourcePicard)\n",
    "    write.table(Picard_qualityMetrics, file=\"${_output}\",col.names=TRUE, row.names=FALSE, quote=FALSE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SoS",
   "language": "sos",
   "name": "sos"
  },
  "language_info": {
   "codemirror_mode": "sos",
   "file_extension": ".sos",
   "mimetype": "text/x-sos",
   "name": "sos",
   "nbconvert_exporter": "sos_notebook.converter.SoS_Exporter",
   "pygments_lexer": "sos"
  },
  "sos": {
   "kernels": [
    [
     "Bash",
     "bash",
     "Bash",
     "#E6EEFF",
     "shell"
    ],
    [
     "SoS",
     "sos",
     "",
     "",
     "sos"
    ]
   ],
   "version": "0.22.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
