{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS",
    "tags": []
   },
   "outputs": [],
   "source": [
    "sos run xqtl-pipeline/code/misc/build_container.ipynb singularity_generator\\\n",
    "    --container_list xqtl-pipeline/code/misc/xQTL_conda_packages1.csv \\\n",
    "    --env bioinfo fastenloc leafcutter METAL methylation polyfun psichomics rna_quantification stephenslab TensorQTL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "sos run xqtl-pipeline/code/misc/build_container.ipynb docker_generator\\\n",
    "    --container_list xqtl-pipeline/code/misc/xQTL_conda_packages1.csv \\\n",
    "    --env bioinfo fastenloc leafcutter METAL methylation polyfun psichomics rna_quantification stephenslab TensorQTL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS",
    "tags": []
   },
   "outputs": [],
   "source": [
    "[global]\n",
    "parameter: container_list = path\n",
    "parameter: env = list\n",
    "parameter: output = env\n",
    "parameter: cwd=path(\"output8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[docker_generator_1, singularity_generator_1]\n",
    "input: container_list, for_each=\"env\"\n",
    "output: f'{cwd}/{_env}/{_env}.yml'\n",
    "python: expand = \"$[ ]\", stderr = f'{_output}.stderr', stdout = f'{_output}.stdout'\n",
    "    import os\n",
    "    import yaml\n",
    "    import pandas as pd\n",
    "\n",
    "    def get_package_and_version(row):\n",
    "        package_name = row['Conda package'].split(\"/\")[-1]\n",
    "        version = row['CondaVersion'] if pd.notnull(row['CondaVersion']) and row['CondaVersion'] != '' else row['Version']\n",
    "        return package_name, f\"{package_name}={version}\" if pd.notna(version) and version != 'nan' else package_name\n",
    "\n",
    "    container_list = pd.read_csv(\"$[_input]\")\n",
    "    container_tempt = dict()\n",
    "\n",
    "    container_list = container_list[container_list.Channel.notna()]\n",
    "    environment_name = \"$[_env]\"\n",
    "    container_tempt[\"name\"] = environment_name\n",
    "    container_tempt[\"channels\"] = list(set(container_list.query(\"Environment.str.contains(@environment_name) | Environment == 'Universial'\").Channel))\n",
    "\n",
    "    subset_list = container_list[(container_list.Environment.str.contains(environment_name)) & (container_list.Channel.isin( container_tempt['channels']))]\n",
    "    dependencies_dict = {package_name: package_version for package_name, package_version in [get_package_and_version(row) for index, row in subset_list.iterrows()]}\n",
    "    \n",
    "\n",
    "    subset_list2 = container_list[(container_list.Environment == 'Universial') & (container_list.Channel.isin(container_tempt['channels']))]\n",
    "    universal_dependencies = [get_package_and_version(row)[1] for index, row in subset_list2.iterrows() if get_package_and_version(row)[0] not in dependencies_dict]\n",
    "\n",
    "    container_tempt[\"dependencies\"] = list(dependencies_dict.values()) + universal_dependencies\n",
    "\n",
    "    container_tempt[\"channels\"].sort(reverse=True)\n",
    "    container_tempt[\"channels\"].append(\"nodefaults\")\n",
    "    container_tempt[\"channels\"].sort(reverse=True)\n",
    "\n",
    "    class IndentDumper(yaml.Dumper):\n",
    "        def increase_indent(self, flow=False, indentless=False):\n",
    "            return super(IndentDumper, self).increase_indent(flow, False)\n",
    "    with open(\"$[_output]\", 'w') as f:\n",
    "        yaml.dump(container_tempt,f,sort_keys = False, Dumper=IndentDumper )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[singularity_generator_2]\n",
    "output: f'{_input:na}.sif'\n",
    "report: output=f'{_input:da}/build.def'\n",
    "    Bootstrap: docker\n",
    "    From: mambaorg/micromamba\n",
    "    %setup\n",
    "        cp *.yml ${SINGULARITY_ROOTFS}\n",
    "    %post\n",
    "        micromamba env create --quiet --yes --file *.yml\n",
    "        micromamba clean --yes\n",
    "bash: expand = \"$[ ]\", stderr = f'{_output}.stderr', stdout = f'{_output}.stdout'\n",
    "    cd $[_input:d]\n",
    "    rm -f $[_output]\n",
    "    singularity build --fakeroot $[_output] build.def\n",
    "    md5sum $[_output] > $[_output].md5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[docker_generator_2]\n",
    "output: f'{_input:na}.tar'\n",
    "report: output=f'{_input:da}/Dockerfile', expand=True\n",
    "    FROM mambaorg/micromamba\n",
    "\n",
    "    COPY {_input:nb}.yml ./\n",
    "\n",
    "    RUN micromamba shell -n base -- bash -c \"micromamba env create --quiet --yes --file {_input:nb}.yml\" && \\\n",
    "        micromamba clean --yes\n",
    "report: output=f'{_input:da}/docker-compose.yml', expand=True\n",
    "    version: '3'\n",
    "    services:\n",
    "      {_input:nb}:\n",
    "        build: {_input:da} # path to directory containing Dockerfile\n",
    "bash: expand = \"$[ ]\", stderr = f'{_output}.stderr', stdout = f'{_output}.stdout'\n",
    "    cd $[_input:d]\n",
    "    docker-compose build\n",
    "    docker save -o $[_output] $[_input:nb]_$[_input:nb]:latest "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SoS",
   "language": "sos",
   "name": "sos"
  },
  "language_info": {
   "codemirror_mode": "sos",
   "file_extension": ".sos",
   "mimetype": "text/x-sos",
   "name": "sos",
   "nbconvert_exporter": "sos_notebook.converter.SoS_Exporter",
   "pygments_lexer": "sos"
  },
  "sos": {
   "kernels": [
    [
     "SoS",
     "sos",
     "",
     ""
    ]
   ],
   "version": "0.24.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
